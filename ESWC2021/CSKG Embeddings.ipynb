{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSKG embeddings\n",
    "\n",
    "This notebook computes similarity between nodes in CSKG and performs grounding of questions/answers to CSKG.\n",
    "We will play with two different families of embeddings: graph and text embeddings.\n",
    "\n",
    "## Graph embeddings \n",
    "\n",
    "The graph embeddings were computed by the using the KGTK `graph-embeddings` as follows:<br>\n",
    "`kgtk graph-embeddings -i < input_file.tsv > -o < output_file.tsv >`<br>\n",
    "Please go to https://kgtk.readthedocs.io/en/latest/analysis/graph_embeddings/ to see more details.\n",
    "\n",
    "## Text embeddings\n",
    "The text embeddings were computed by using the KGTK `text-embedding` command as follows:<br>\n",
    "`kgtk text-embedding < input_file.tsv > < output_file.tsv >`<br>\n",
    "Please go to https://kgtk.readthedocs.io/en/latest/analysis/text_embedding/ to see more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for working with embeddings\n",
    "\n",
    "## Parameters for invoking the notebook\n",
    "\n",
    "- `cskg_path`: a folder containing the necessary files and all the analysis products.\n",
    "- `graph_emb`: the name of the graph embedding output file\n",
    "- `text_emb`: the name of the text embedding output file\n",
    "- `distance`: measurement for embedding distance\n",
    "\n",
    "\n",
    "Tip: Since it takes much time to generate graph embeddings and text embeddings, We have prepared the `graph_emb` and `text_emb` in advance. You can download them from https://drive.google.com/drive/u/1/folders/16347KHSloJJZIbgC9V5gH7_pRx0CzjPQ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cskg_path = \"../output\" \n",
    "graph_emb = \"trans_log_dot_0.1.tsv.gz\"\n",
    "text_emb = \"bert-nli-large-embeddings.tsv.gz\"\n",
    "distance='cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Callable, List, Tuple\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CSKG'] = cskg_path\n",
    "os.environ['GE'] = \"{}/{}\".format(cskg_path, graph_emb)\n",
    "os.environ['TE'] = \"{}/{}\".format(cskg_path, text_emb)\n",
    "graph_emb_path = os.environ['GE']\n",
    "text_emb_path = os.environ['TE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../output\n",
      "../output/trans_log_dot_0.1.tsv.gz\n",
      "../output/bert-nli-large-embeddings.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!echo $CSKG\n",
    "!echo $GE\n",
    "!echo $TE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, words) -> None:\n",
    "        self.idx_to_word = words\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "def read_embedding_file(embedding_file: Path, dim: int, emb_col=1) -> Tuple[Vocab, np.ndarray]:\n",
    "\n",
    "    logger.debug(f'Reading embeddings from {embedding_file}')\n",
    "\n",
    "    shape = tuple([count_lines(embedding_file), dim])\n",
    "                  \n",
    "    with gzip.open(embedding_file, 'r') as f:\n",
    "\n",
    "        embeddings = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "        if emb_col!=1:\n",
    "            header=next(f)\n",
    "        i=0\n",
    "        words = []\n",
    "        for line in tqdm(f, total=shape[0]):\n",
    "            line=line.decode()\n",
    "            if emb_col==1:\n",
    "                node, *embedding = line.split()\n",
    "            else:\n",
    "                line_data=line.split()\n",
    "                if line_data[1]=='embedding_sentence': continue\n",
    "                node=line_data[0]\n",
    "                embedding=line_data[2].split(',')\n",
    "            embedding = np.array([float(x) for x in embedding])\n",
    "            words.append(node)\n",
    "            embeddings[i] = embedding\n",
    "            i+=1\n",
    "\n",
    "    vocab = Vocab(words)\n",
    "\n",
    "    return vocab, embeddings\n",
    "\n",
    "def count_lines(embedding_file: Path):\n",
    "    with gzip.open(embedding_file, 'r') as f:\n",
    "        i=0\n",
    "        for line in f:\n",
    "            i+=1\n",
    "    return i\n",
    "\n",
    "def build_index(metric: str, embeddings: np.ndarray):\n",
    "\n",
    "    logger.debug(f'Building search index')\n",
    "\n",
    "    if metric == 'cosine':\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[-1])\n",
    "    elif metric == 'l2':\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[-1])\n",
    "    else:\n",
    "        raise ValueError(f'Bad metric: {metric}')\n",
    "\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2160968/2160968 [01:17<00:00, 27865.38it/s]\n"
     ]
    }
   ],
   "source": [
    "graph_dim = 100 # Dimension of the graph embeddings for our example's file\n",
    "graph_vocab, graph_embeddings = read_embedding_file(graph_emb_path,graph_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_index = build_index(distance, graph_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2161048/2161049 [10:44<00:00, 3355.01it/s]\n"
     ]
    }
   ],
   "source": [
    "text_dim=1024 # Dimension of the text embeddings for our example's file\n",
    "text_vocab, text_embeddings = read_embedding_file(text_emb_path, text_dim, emb_col=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index = build_index(distance, text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Most similar nodes in CSKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_nodes=['/c/en/turtle', '/c/en/happy', '/c/en/turtle/n/wn/animal', \n",
    "             'at:personx_abandons_____altogether', '/c/en/caffeine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbors=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors to */c/en/turtle*\n",
      "['/c/en/tortoise', '/c/en/turtling/v/wikt/en_1', '/c/en/turtles/v', '/c/en/turtled/v', '/c/en/turtles/n']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/happy*\n",
      "['/c/en/pleased', '/c/en/excited', '/c/en/content', '/c/en/satisfied', '/c/en/joyful']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/turtle/n/wn/animal*\n",
      "['/c/en/chelonian/n/wn/animal', '/c/en/sea_turtle/n/wn/animal', '/c/en/pseudemys/n/wn/animal', '/c/en/mud_turtle/n/wn/animal', '/c/en/cooter/n/wn/animal']\n",
      "\n",
      "\n",
      "Nearest neighbors to *at:personx_abandons_____altogether*\n",
      "['at:turns_over_a_new_leaf', 'at:to_get_permission_from_his_parents', 'at:plows_the_field', 'at:was_just_city', 'at:to_search_for_a_new_job']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/caffeine*\n",
      "['/c/en/caffiene/n', '/c/en/caffeines/n', '/c/en/caffein/n', '/c/en/noncaffeine', '/c/en/caffeinelike']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids=[graph_vocab.word_to_idx[n] for n in query_nodes]\n",
    "distances, neighbors = graph_index.search(graph_embeddings[ids], num_neighbors+1)\n",
    "for node_nbrs in neighbors:\n",
    "    neighboring_nodes=[graph_vocab.idx_to_word[n] for n in node_nbrs]\n",
    "\n",
    "    print('Nearest neighbors to *%s*' % neighboring_nodes[0])\n",
    "    print(neighboring_nodes[1:])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors to */c/en/turtle*\n",
      "['/c/en/shrimp_and_turtle', '/c/en/turtles', '/c/en/dolphin', '/c/en/loon/n/wn/animal', '/c/en/ducks']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/happy*\n",
      "['/c/en/bring_happiness', 'at:happy_that_they_went_to_the_party', 'at:like_a_party_is_a_good_way_to_express_their_jubilation', 'at:if_for_a_party,_happy', \"/c/en/encouraging_person's_talent\"]\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/turtle/n/wn/animal*\n",
      "['/c/en/glyptemys/n', '/c/en/chelidae/n', '/c/en/pelocomastes/n', '/c/en/parahydraspis/n', '/c/en/sternotherus/n']\n",
      "\n",
      "\n",
      "Nearest neighbors to *at:personx_abandons_____altogether*\n",
      "['at:personx_is_sent_home', \"at:personx_loses_personx's_position\", 'at:personx_loses_persony_opportunity', 'at:personx_loses_a_bet', 'at:personx_is_promptly_fired']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/caffeine*\n",
      "['/c/en/people_drink_coffee_because', '/c/en/cup_of_coffee', '/c/en/brewing_coffee/n', 'at:put_coffee_grounds_in_caffee_maker', '/c/en/hot_chocolate']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids=[text_vocab.word_to_idx[n] for n in query_nodes]\n",
    "distances, neighbors = text_index.search(text_embeddings[ids], num_neighbors+1)\n",
    "\n",
    "for node_nbrs in neighbors:\n",
    "    neighboring_nodes=[text_vocab.idx_to_word[n] for n in node_nbrs]\n",
    "\n",
    "    print('Nearest neighbors to *%s*' % neighboring_nodes[0])\n",
    "    print(neighboring_nodes[1:])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Calculate similarity between two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pairs=[['/c/en/woman', '/c/en/man'], \n",
    "            ['/c/en/pencil', 'Q614304'], \n",
    "            ['/c/en/ash', 'rg:en_ash-gray'], \n",
    "            ['/c/en/spiritual', '/c/en/religion'], \n",
    "            ['/c/en/monkey', '/c/en/gorilla'], \n",
    "            ['/c/en/monkey', '/c/en/tea']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/en/woman /c/en/man 0.62742233\n",
      "/c/en/pencil Q614304 0.028264966\n",
      "/c/en/ash rg:en_ash-gray 0.30954424\n",
      "/c/en/spiritual /c/en/religion 0.4183224\n",
      "/c/en/monkey /c/en/gorilla 0.46226677\n",
      "/c/en/monkey /c/en/tea 0.24436088\n"
     ]
    }
   ],
   "source": [
    "for nodes in node_pairs:\n",
    "    ids=[graph_vocab.word_to_idx[n] for n in nodes]\n",
    "    ge=graph_embeddings[ids]\n",
    "    print(' '.join(nodes), cosine_similarity([ge[0]], [ge[1]])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/en/woman /c/en/man 0.32196832\n",
      "/c/en/pencil Q614304 0.43661886\n",
      "/c/en/ash rg:en_ash-gray 0.5951916\n",
      "/c/en/spiritual /c/en/religion 0.54427576\n",
      "/c/en/monkey /c/en/gorilla 0.75078595\n",
      "/c/en/monkey /c/en/tea 0.48101428\n"
     ]
    }
   ],
   "source": [
    "for nodes in node_pairs:\n",
    "    ids=[text_vocab.word_to_idx[n] for n in nodes]\n",
    "    ge=text_embeddings[ids]\n",
    "    print(' '.join(nodes), cosine_similarity([ge[0]], [ge[1]])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
