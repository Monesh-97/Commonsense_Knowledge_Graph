{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation between CSKG and USF-FAN\n",
    "\n",
    "This notebook performs evaluation ranking between cksg and USF-FAN, it will calculate MAP and MPR for different entity embedding gz files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import faiss\n",
    "import gzip\n",
    "import os\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for invoking the notebook\n",
    "\n",
    "- `cue_target`: a xml file contains the grounding truth of USF-FAN dataset\n",
    "- `cskg_connected`: a tsv file contains the raw cskg entity information\n",
    "- `embedding_folder`: a folder contains all of the embedding gz files\n",
    "- `evaluation_res`: a folder contains MAP@K and MPR result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters\n",
    "cue_target = '../input/cue-target.xml'\n",
    "cskg_connected = '../input/cskg_connected.tsv'\n",
    "embedding_folder = '../output/embeddings'\n",
    "evaluation_res = '../output/evaluation_res'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Utils\n",
    "\n",
    "- `dict_to_json(dict_,output_file)`:  convert dictionary to json file\n",
    "- `get_file_path(embedding_folder)`:  get all of embedding gz files \n",
    "- `get_max_num(ground_truth_dict)`: get the number of ground_truth_dict's k-v pair whose v has largest length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_json(dict_,output_file):\n",
    "    with open(output_file,'w') as f:\n",
    "        json.dump(dict_,f)\n",
    "         \n",
    "def get_file_path(embedding_folder):\n",
    "    gz_list = []\n",
    "    for gz_file in os.listdir(embedding_folder):\n",
    "        file_path = os.path.join(embedding_folder, gz_file)\n",
    "        gz_list.append(file_path)   \n",
    "    return gz_list\n",
    "    \n",
    "def get_max_num(ground_truth_dict):\n",
    "    max_num = 0\n",
    "    for label in ground_truth_dict:\n",
    "         if max_num < len(ground_truth_dict[label]):\n",
    "                max_num = len(ground_truth_dict[label])\n",
    "    return max_num       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prepara data\n",
    "\n",
    "- `xml_load(input_file)`: load USF-FAN dataset and convert it into dictionary format\n",
    "- `create_cskg_index(tsv_file)`: load cskg_connected.tsv and convert it into dictionary format\n",
    "- `load_ent_embeddings(input_file)`: load entity embeddings and create two dictionaries to store node index and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_load(input_file):   # cue-target.xml'\n",
    "    tree = etree.parse(input_file)\n",
    "    root = tree.getroot()\n",
    "    # create a dict to store ground truth sets, \n",
    "    # example : `p={'car': ['wheel', 'driver', ...], 'book`: [...]}`\n",
    "    ground_truth = {}\n",
    "    for cue_ele in root:\n",
    "        key = cue_ele.get('word').lower()\n",
    "        ground_truth[key] = []\n",
    "        for word_ele in cue_ele:\n",
    "            ground_truth[key].append(word_ele.get('word').lower())\n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "def create_cskg_index(tsv_file): # cskg_connected.tsv\n",
    "    cskg_index_dict = {}\n",
    "    #  create a dict to store cskg data set   label: node_list\n",
    "    #  example : `p = {'turtle':  ['Q1705322', '/c/en/turtle', ...], 'book': [...]}`\n",
    "    with open(tsv_file) as f:\n",
    "        for line in f:\n",
    "            content = line.split('\\t')\n",
    "            if content[0]!='id': # ignore the first time \n",
    "                node1_id = content[1]\n",
    "                node2_id = content[3]\n",
    "                node1_lbl = content[4]\n",
    "                node2_lbl = content[5]\n",
    "                cskg_index_dict[node1_lbl] = cskg_index_dict.get(node1_lbl,set())\n",
    "                cskg_index_dict[node1_lbl].add(node1_id)\n",
    "                cskg_index_dict[node2_lbl] = cskg_index_dict.get(node2_lbl,set())\n",
    "                cskg_index_dict[node2_lbl].add(node2_id)\n",
    "                \n",
    "    # convert set to list\n",
    "    for k in cskg_index_dict:\n",
    "        cskg_index_dict[k] = list(cskg_index_dict[k])\n",
    "\n",
    "    return cskg_index_dict\n",
    "\n",
    "def load_ent_embeddings(input_file):\n",
    "    # input file folder path :/nas/home/binzhang/backup_data/embeddings \n",
    "    #  create a dict to store cskg embeddings   node: embedding example: '/c/en/turtle': [0.01,0.02....]\n",
    "\n",
    "    ix_node_dict = {} # { node_index: node_name,... node_name:node_index... }\n",
    "    node_embedding_dict = {} # {node_name:embedding, ....}\n",
    "    with gzip.open(input_file,'rt') as f:\n",
    "        for index,line in enumerate(f):\n",
    "            line = line.split('\\t')\n",
    "            entity_name = line[0]\n",
    "            entity_vec =  [ float(i) for i in line[1:]]\n",
    "            ix_node_dict[entity_name] = index\n",
    "            ix_node_dict[index] = entity_name\n",
    "            node_embedding_dict[entity_name] = entity_vec\n",
    "    \n",
    "    return ix_node_dict,node_embedding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data\n",
    "\n",
    "- `cal_avg_embeddings(node_embedding_dict,cskg_index_dict)`: calculate entity's average embedding\n",
    "- `build_fassi_index(avg_embeddings)`: build a fassi index for embedding matrix and a label dictionary for each entity label\n",
    "- `create_queryset(ground_truth_dict,label_dict,avg_embeddings)`: build query set for fassi index\n",
    "- `neighbor_searching(vec_index,query_ent_mat,query_ent_dict,label_dict,k,fix_num)`: find neighbors of query set\n",
    "- `map_at_k(pre_dict,grouding_dict,k)`: compare pre_dict(cskg) and grouding_dict(USF-FAN) to get MAP\n",
    "- `MPR(pre_dict,grouding_dict):`: compare pre_dict(cskg) and grouding_dict(USF-FAN) to get MPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_avg_embeddings(node_embedding_dict,cskg_index_dict):\n",
    "    # node_embedding_dict's key is node's name (example: '/c/en/joke') and the value is the embedding vectors\n",
    "    # example: '/c/en/turtle': [0.01,0.02...]\n",
    "\n",
    "    # cskg_index_dict's key is the label for a node , value is a list recording the node's name\n",
    "    # example : 'joke': ['/c/en/joke', '/c/en/joke/n', '/c/en/joke/n/wn/act',...]\n",
    "    avg_embeddings = {}\n",
    "    for label in cskg_index_dict:\n",
    "        entity_names = cskg_index_dict[label]\n",
    "        size = len(entity_names)\n",
    "        sum_embedding =  node_embedding_dict[entity_names[0]]\n",
    "        for entity in entity_names[1:]:\n",
    "            embedding = node_embedding_dict[entity] # embeddings list \n",
    "            sum_embedding = list(map(lambda x,y : x+y ,sum_embedding,embedding))\n",
    "            \n",
    "        avg_emb = [i/size for i in sum_embedding]\n",
    "        avg_embeddings[label] = avg_emb\n",
    "        \n",
    "    return avg_embeddings\n",
    "\n",
    "def build_fassi_index(avg_embeddings):\n",
    "    # avg_embeddings is a dictionary which key is the node label and value is lable's embedding\n",
    "    \n",
    "    label_dict = {}         # build a entity label-index bi dictionary\n",
    "    entity_embeddings = []  # all the embeddings \n",
    "\n",
    "    index = 0\n",
    "    for key,value in avg_embeddings.items():\n",
    "        label_dict[index] = key\n",
    "        label_dict[key] = index\n",
    "        index+=1    \n",
    "        entity_embeddings.append(value)\n",
    "\n",
    "    # entity_embeddings => matrix  X contains  all labels' embeddings \n",
    "    X = np.array(entity_embeddings).astype(np.float32) # float32\n",
    "    dimension = X.shape[1]\n",
    "\n",
    "    # build index (METRIC_INNER_PRODUCT => cos )\n",
    "    vec_index = faiss.index_factory(dimension, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "    # # normalize all vectors in order to get cos sim \n",
    "    faiss.normalize_L2(X)  \n",
    "    # add vectors to inde \n",
    "    vec_index.add(X) \n",
    "    \n",
    "    return vec_index,label_dict\n",
    "\n",
    "def create_queryset(ground_truth_dict,label_dict,avg_embeddings):\n",
    "    query_ent_vecs = []\n",
    "    query_ent_dict = {}\n",
    "    miss_concept = 0\n",
    "    miss_concept_list = []\n",
    "    \n",
    "    for key in ground_truth_dict:\n",
    "        if key in label_dict: \n",
    "            query_ent_dict[len(query_ent_vecs)] = key\n",
    "            query_ent_dict[key] = len(query_ent_vecs)\n",
    "            query_ent_vecs.append(avg_embeddings[key])\n",
    "        else:\n",
    "            miss_concept_list.append(key)\n",
    "            miss_concept+=1\n",
    "\n",
    "    query_ent_mat = np.array(query_ent_vecs).astype(np.float32)\n",
    "    faiss.normalize_L2(query_ent_mat) \n",
    "    \n",
    "    return query_ent_mat,query_ent_dict\n",
    "\n",
    "\n",
    "def neighbor_searching(vec_index,query_ent_mat,query_ent_dict,label_dict,k,fix_num):\n",
    "    # k = times of items => k = 1 ,reterice @1 items k = 3 ,reterice @3 items \n",
    "\n",
    "    neigh_num = k*fix_num\n",
    "    cos_sim, index = vec_index.search(query_ent_mat, neigh_num)     # both of them are matrices\n",
    "\n",
    "    neighbors_dict = {}\n",
    "    for ix,neighbors in enumerate(index):\n",
    "        query_item = query_ent_dict[ix]\n",
    "        tmp_list = []\n",
    "        for id_ in neighbors:\n",
    "            tmp_list.append(label_dict[id_])            # ix refers to the label's index \n",
    "\n",
    "        neighbors_dict[query_item] = tmp_list\n",
    "\n",
    "    return neighbors_dict   \n",
    "\n",
    "\n",
    "## evaluation metric\n",
    "def apk(actual, predicted, k):   \n",
    "    # keep predicted's order igonore actual's order\n",
    "    if len(predicted)>k*len(actual):\n",
    "        predicted = predicted[:k]\n",
    "    ap = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            ap += num_hits / (i+1.0)\n",
    "\n",
    "    if num_hits == 0:  # no match from predict and actual \n",
    "        return 0.0\n",
    "    else:\n",
    "        return ap / num_hits\n",
    "    \n",
    "def map_at_k(pre_dict,grouding_dict,k):\n",
    "    MAP = 0 \n",
    "    set_size = len(pre_dict) \n",
    "\n",
    "    # cal ap\n",
    "    for label in pre_dict:\n",
    "        predicted = pre_dict.get(label,[])\n",
    "        actual = grouding_dict.get(label,[])\n",
    "        ap = apk(actual, predicted, k)\n",
    "        MAP+=ap\n",
    "\n",
    "    return MAP/set_size\n",
    "    \n",
    "    \n",
    "def reci_rank(actual, predicted):\n",
    "    # The inverse of the ranking of the first correct answer\n",
    "    # keep both predicted's order and actual's order\n",
    "    for i in  predicted:\n",
    "        if i in actual:\n",
    "            return 1/(actual.index(i)+1)\n",
    "\n",
    "    return 0 # no match     \n",
    "\n",
    "def MPR(pre_dict,grouding_dict):\n",
    "    MPR = 0\n",
    "    set_size = len(pre_dict)\n",
    "\n",
    "    for label in pre_dict:\n",
    "        predicted = pre_dict.get(label,[])\n",
    "        actual = grouding_dict.get(label,[])\n",
    "        rr = reci_rank(actual, predicted)\n",
    "        MPR+=rr\n",
    "\n",
    "    return MPR/set_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedures\n",
    "1. prepares  USF-FAN dataset into dictionary  result: ground_truth_dict\n",
    "2. construct an index of CSKG from label to node id  result: cskg_index_dict\n",
    "3. get all possible possible embeddings from embedding folder\n",
    "4. For each possible embedding:\n",
    "    - obtain the embeddings for all entity.\n",
    "    - compute an average embedding.\n",
    "    - create a fassi vector index.\n",
    "    - create query sets based on grounding truth(USF-FAN).\n",
    "    - set topk and do calculate MAP and MPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['good-bye', 'hi', 'greeting', 'again', 'jello', 'phone', 'smile'], 34)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load  USF-FAN dataset \n",
    "ground_truth_dict = xml_load(cue_target)\n",
    "max_num = get_max_num(ground_truth_dict)\n",
    "ground_truth_dict['hello'], max_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/c/en/hello/v',\n",
       " '/c/en/hello/n/wp/airline',\n",
       " '/c/en/hello',\n",
       " 'Q59944576',\n",
       " '/c/en/hello/n/wp/good_to_be_back',\n",
       " '/c/en/hello/n/wn/communication',\n",
       " '/c/en/hello/n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load CSKG data\n",
    "# construct an index of CSKG from label to node id  result: cskg_index_dict\n",
    "cskg_index_dict = create_cskg_index(cskg_connected)\n",
    "cskg_index_dict['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../output/embeddings/comp_log_cos_0.01.tsv.gz'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all possible possible embeddings from  embedding_folder\n",
    "gz_list = get_file_path(embedding_folder)\n",
    "gz_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding gz file: ../output/embeddings/comp_log_cos_0.01.tsv.gz as an example\n",
      "CPU times: user 2min 23s, sys: 14.2 s, total: 2min 38s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "###  Here we take an embedding gz file as an example, do MAP and MPR calculation\n",
    "print(f'Using embedding gz file: {gz_list[0]} as an example')\n",
    "\n",
    "# obtain the embeddings for all concepts \n",
    "ix_node_dict,node_embedding_dict = load_ent_embeddings(gz_list[0])\n",
    "# compute an average embedding. \n",
    "avg_embeddings = cal_avg_embeddings(node_embedding_dict,cskg_index_dict)\n",
    "\n",
    "# faiss: create vector index\n",
    "vec_index,label_dict= build_fassi_index(avg_embeddings)\n",
    "\n",
    "# create query sets based on grounding truth(USF-FAN)\n",
    "query_ent_mat,query_ent_dict = create_queryset(ground_truth_dict,label_dict,avg_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neigbors for 'hello' on cskg: ['hello', 'tonight', 'wipeout', 'young buck', 'trap door', 'miseration', 'down town', 'jr', 'golfing', 'water bearer']...\n",
      "\n",
      "Neighbors for 'hello' on USF-FAN: ['good-bye', 'hi', 'greeting', 'again', 'jello', 'phone', 'smile']\n"
     ]
    }
   ],
   "source": [
    "k = 3 # means we will calculate MAP@1\n",
    "neighbors_dict = neighbor_searching(vec_index,query_ent_mat,query_ent_dict,label_dict,k,max_num)\n",
    "print(f\"Neigbors for 'hello' on cskg: {neighbors_dict['hello'][:10]}...\")\n",
    "print()\n",
    "print(f\"Neighbors for 'hello' on USF-FAN: {ground_truth_dict['hello']}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@3: 0.01719137482610536\n",
      "MPR@3: 0.07766852836628571\n"
     ]
    }
   ],
   "source": [
    "# calculat map@k and mpr\n",
    "MAP = map_at_k(neighbors_dict,ground_truth_dict,k)\n",
    "print(f'MAP@3: {MAP}')\n",
    "mpr = MPR(neighbors_dict,ground_truth_dict)\n",
    "print(f'MPR@3: {mpr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to get all possible results, you can and make a for loop to execute \n",
    "It may take much time(>16h) here I just use 2 and two tsv files  as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [03:14<03:14, 194.77s/it]\u001b[A\n",
      "100%|██████████| 2/2 [06:07<00:00, 183.97s/it]\u001b[A\n",
      "100%|██████████| 1/1 [06:07<00:00, 367.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@2 has been calculated for all embeddings\n",
      "MPR@2 has been calculated for all embeddings\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "K = [2] # [1,2,3,5,10]\n",
    "for k in tqdm(K,total=len(K)):\n",
    "    MAPs,MPRs  = {},{} \n",
    "    # assign output file path\n",
    "    map_out =  f'{evaluation_res}/MAP@{k}.json'\n",
    "    mpr_out =  f'{evaluation_res}/MPR@{k}.json'\n",
    "    \n",
    "    for ent_embedding_path in tqdm(gz_list[:2],total=len(gz_list[:2])):  # tqdm(gz_list,total=len(gz_list)): \n",
    "        # obtain the embeddings for all concepts \n",
    "        ix_node_dict,node_embedding_dict = load_ent_embeddings(ent_embedding_path)        \n",
    "        # compute an average embedding. \n",
    "        avg_embeddings = cal_avg_embeddings(node_embedding_dict,cskg_index_dict)\n",
    "        # faiss: create vector index\n",
    "        vec_index,label_dict= build_fassi_index(avg_embeddings)\n",
    "        # create query sets based on grounding truth(USF-FAN)\n",
    "        query_ent_mat,query_ent_dict = create_queryset(ground_truth_dict,label_dict,avg_embeddings)\n",
    "        \n",
    "        # do neighbor searching\n",
    "        neighbors_dict = neighbor_searching(vec_index,query_ent_mat,query_ent_dict,label_dict,k,max_num)\n",
    "            \n",
    "        #calculate metrics\n",
    "        MAP = map_at_k(neighbors_dict,ground_truth_dict,k)\n",
    "        mpr = MPR(neighbors_dict,ground_truth_dict)\n",
    "        emb_key = ent_embedding_path.split('/')[-1]\n",
    "        MAPs[emb_key] = MAP\n",
    "        MPRs[emb_key] = mpr\n",
    "          \n",
    "    dict_to_json(MAPs,map_out)\n",
    "    dict_to_json(MPRs,mpr_out)  \n",
    "    print(f'MAP@{k} has been calculated for all embeddings')\n",
    "    print(f'MPR@{k} has been calculated for all embeddings')\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:\n",
    "\n",
    "MAP@2 =>{\"comp_log_cos_0.01.tsv.gz\": 0.013581927054980947, \"comp_log_cos_0.05.tsv.gz\": 0.006586826347305389} \n",
    "\n",
    "MPR@2 =>{\"comp_log_cos_0.01.tsv.gz\": 0.0659998486415124, \"comp_log_cos_0.05.tsv.gz\": 0.04782341352235148}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
