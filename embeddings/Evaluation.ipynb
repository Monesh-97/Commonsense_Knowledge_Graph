{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook that evaluates predictions generated by `Predict with CSKG.ipynb`\n",
    "Now there two types of prediction: graph_preditons and text_predictions\n",
    "\n",
    "TODO... looking into more complex neural network-based solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation' from '/data/cskg/embeddings/evaluation.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluation # a script contains related function for do evaluations\n",
    "import importlib\n",
    "importlib.reload(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for invoking the notebook\n",
    "\n",
    "- `trues`: file path of trues.json (conatins the validation data for evaluation)\n",
    "- `graph_pred`: file path of graph_predictions.json (predictions generated by graph embedding and faiss)\n",
    "- `text_pred`: file path of text_predictions.json (predictions generated by text embedding and faiss)\n",
    "- `modi_text_pred`: file path of text_predictions.json , here the some targets for label have been ignored\n",
    "\n",
    "\n",
    "- `graph_pred_50X`: file path of graph_predictions_50X.json, targets' size 50 times as the ground turth => for NDCG calculation\n",
    "- `text_pred_50X`: file path of text_predictions__50X.json, targets' size 50 times as the ground turth => for NDCG calculation\n",
    "- `modi_text_pred_50X`: file path of modi_text_predictions_50X.json, targets' size 50 times as the ground turth => for NDCG calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##output\n",
    "trues = '../output/trues.json'\n",
    "graph_pred  = '../output/graph_predictions.json'\n",
    "text_pred  = '../output/text_predictions.json'\n",
    "modi_text_pred  = '../output/modi_text_predictions.json'\n",
    "\n",
    "graph_pred_50X  = '../output/graph_predictions_50X.json'\n",
    "text_pred_50X  = '../output/text_predictions__50X.json'\n",
    "modi_text_pred_50X = '../output/modi_text_predictions_50X.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ground truth and predictions\n",
    "\n",
    "variables:\n",
    "\n",
    "- `ground_truth`: A dictionary whose key is both in USF_FAN and CSKG, value the same value as the USF_FAN_dict for the cue, this is used as the gold_list\n",
    "- `graph_predictions`: A dictionary with same key with ground_truth , but the value is the list of neighbors generated by faiss searching according to the cue's graph embeddings\n",
    "- `text_predictions`: A dictionary with same key with ground_truth , but the value is the list of neighbors generated by faiss searching according to the cue's text embeddings\n",
    "- `modi_text_predictions`: A dictionary with same key with ground_truth , but the value is the list of neighbors generated by faiss searching according to the cue's text embeddings (some targets for label have been ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = evaluation.load_cue_targets(trues)\n",
    "graph_predictions = evaluation.load_cue_targets(graph_pred)\n",
    "graph_predictions_50X = evaluation.load_cue_targets(graph_pred_50X)\n",
    "text_predictions = evaluation.load_cue_targets(text_pred)\n",
    "text_predictions_50X = evaluation.load_cue_targets(text_pred_50X)\n",
    "modi_text_predictions = evaluation.load_cue_targets(modi_text_pred)\n",
    "modi_text_predictions_50X = evaluation.load_cue_targets(modi_text_pred_50X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different evaluation metrics\n",
    "\n",
    "- `MAP`:  inform us how many of the top candidates are good. \n",
    "- `MRR`:  inform us about the ranking by the system for the top answer\n",
    "- `Hits`: inform us about the number of good results in flexible number of top result, here we can use either 'mircro' or 'macro' to get the result\n",
    "- `NDCG`: inform us about the relative ranking of the positive results between the system and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For grpah prediction @X\n",
      "hit_micro: 0.0729793935344536\n",
      "hit_macro: 0.06919887395820333\n",
      "MAP: 0.2418847995261804\n",
      "MRR: 0.07591513776394637\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics between ground truth and graph predictions\n",
    "hits_micro_graph = evaluation.cal_hits(ground_truth,graph_predictions,level='micro')\n",
    "hits_macro_graph = evaluation.cal_hits(ground_truth,graph_predictions,level='macro')\n",
    "MAP_graph = evaluation.cal_map(ground_truth,graph_predictions)\n",
    "MRR_graph = evaluation.cal_mrr(ground_truth,graph_predictions)\n",
    "print(\"For grpah prediction @X\")\n",
    "print(f\"hit_micro: {hits_micro_graph}\")\n",
    "print(f\"hit_macro: {hits_macro_graph}\")\n",
    "print(f\"MAP: {MAP_graph}\")\n",
    "print(f\"MRR: {MRR_graph}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For text prediction @X\n",
      "hit_micro: 0.02748779102733233\n",
      "hit_macro: 0.0272773917987547\n",
      "MAP: 0.09015168588218885\n",
      "MRR: 0.021420157065854165\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics between ground truth and text predictions(no disgarding targets)\n",
    "hits_micro = evaluation.cal_hits(ground_truth,text_predictions,level='micro')\n",
    "hits_macro = evaluation.cal_hits(ground_truth,text_predictions,level='macro')\n",
    "MAP = evaluation.cal_map(ground_truth,text_predictions)\n",
    "MRR = evaluation.cal_mrr(ground_truth,text_predictions)\n",
    "print(\"For text prediction @X\")\n",
    "print(f\"hit_micro: {hits_micro}\")\n",
    "print(f\"hit_macro: {hits_macro}\")\n",
    "print(f\"MAP: {MAP}\")\n",
    "print(f\"MRR: {MRR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For modified text prediction @X\n",
      "hit_micro: 0.05155035311437886\n",
      "hit_macro: 0.04977049271262359\n",
      "MAP: 0.20860608529952573\n",
      "MRR: 0.06210292184757792\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics between ground truth and text predictions(some targets have been ignored)\n",
    "hits_micro = evaluation.cal_hits(ground_truth,modi_text_predictions,level='micro')\n",
    "hits_macro = evaluation.cal_hits(ground_truth,modi_text_predictions,level='macro')\n",
    "MAP = evaluation.cal_map(ground_truth,modi_text_predictions)\n",
    "MRR = evaluation.cal_mrr(ground_truth,modi_text_predictions)\n",
    "print(\"For modified text prediction @X\")\n",
    "print(f\"hit_micro: {hits_micro}\")\n",
    "print(f\"hit_macro: {hits_macro}\")\n",
    "print(f\"MAP: {MAP}\")\n",
    "print(f\"MRR: {MRR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When it comes to NCGC, things are different, here we don't use @X but use as many as possible targets to make sure\n",
    "# that the targets in ground turth can be found in the preditions, so we use other predictions (@50X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG for graph prediction: 0.5303820347468684\n"
     ]
    }
   ],
   "source": [
    "NDCG = evaluation.cal_ndcg(ground_truth,graph_predictions_50X)\n",
    "print(f\"NDCG for graph prediction: {NDCG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG for text prediction: 0.298756138043906\n"
     ]
    }
   ],
   "source": [
    "NDCG = evaluation.cal_ndcg(ground_truth,text_predictions_50X)\n",
    "print(f\"NDCG for text prediction: {NDCG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG for modified text prediction: 0.2791628182368776\n"
     ]
    }
   ],
   "source": [
    "NDCG = evaluation.cal_ndcg(ground_truth,modi_text_predictions_50X)\n",
    "print(f\"NDCG for modified text prediction: {NDCG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
