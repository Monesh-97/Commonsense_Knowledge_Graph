{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation between Bert text and USF-FAN\n",
    "\n",
    "This notebook performs evaluation ranking between  BERT-NLI-large text and USF-FAN, it mainly contains two parts:\n",
    "* Cap label with cskg's node\n",
    "* Calculate MAP(Mean Average Precision) and MPR(Mean Reciprocal Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for invoking the notebook\n",
    "\n",
    "- `cue_target`: a xml file contains the grounding truth of USF-FAN dataset\n",
    "- `cskg_connected`: a tsv file contains the raw cskg entity information\n",
    "- `bert_embs`: a tsv file in .gz format contain the BERT-NLI-large text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_target = '../input/cue-target.xml'\n",
    "cskg_connected = '../input/cskg_connected.tsv'\n",
    "bert_embs = '../input/bert-nli-large-embeddings.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class for data preparation\n",
    "\"\"\"\n",
    "class DataLoader():\n",
    "    def __init__(self,cue_target,cskg_connected,bert_embs):\n",
    "        \"\"\"\n",
    "        Parameters for invoking the notebook\n",
    "        cue_target : a xml file contains the grounding truth of USF-FAN dataset\n",
    "        bert_embs: a gzip(tsv) file contains the raw text's embedding information\n",
    "        cskg_connected: a tsv file contains the raw cskg entity information\n",
    "        embedding_folder: a folder contains all of the embedding cskg gz files\n",
    "        MAP_res: a json file contains the MAP result for each cskg embedding gz file\n",
    "        \"\"\"\n",
    "        self.cue_target = cue_target # '../input/cue-target.xml'\n",
    "        self.bert_embs = bert_embs # '../input/bert-nli-large-embeddings.tsv.gz'\n",
    "        self.cskg_connected = cskg_connected # '../input/cskg_connected.tsv'\n",
    "        self.actual_max_num = 0\n",
    "\n",
    "    ######################## USF-FAN loading #####################################\n",
    "    def xml_load(self):   # cue-target.xml'\n",
    "        cue_target = self.cue_target\n",
    "        tree = etree.parse(cue_target)\n",
    "        root = tree.getroot()\n",
    "        # create a dict to store ground truth sets, \n",
    "        # example : `p={'car': ['wheel', 'driver', ...], 'book`: [...]}`\n",
    "        ground_truth = {}\n",
    "        for cue_ele in root:\n",
    "            key = cue_ele.get('word').lower()\n",
    "            ground_truth[key] = []\n",
    "            for word_ele in cue_ele:\n",
    "                ground_truth[key].append(word_ele.get('word').lower())\n",
    "                \n",
    "        # get the max_num of atcual items\n",
    "        for items in ground_truth.values():\n",
    "            if self.actual_max_num < len(items):\n",
    "                self.actual_max_num = len(items)\n",
    "                \n",
    "        return ground_truth\n",
    "\n",
    "    ######################## BERT large text loading #####################################\n",
    "    def bert_load(self,file_length=2161049): # '../input/bert-nli-large-embeddings.tsv.gz'\n",
    "        bert_embs = self.bert_embs\n",
    "\n",
    "        text_embed_dict= {}\n",
    "        with gzip.open(bert_embs, 'rb') as f:\n",
    "            for line in tqdm(f,total=file_length): # prerun it to get the total number 2161049\n",
    "                line = line.decode()\n",
    "                node,prop,value = line.split('\\t')\n",
    "                value = value.split(',')\n",
    "                if node == 'node': # ignore the first line \n",
    "                    continue \n",
    "                embedding = [ float(i) for i in value]\n",
    "                text_embed_dict[node] = embedding \n",
    "                \n",
    "        return text_embed_dict\n",
    "\n",
    "    ######################## CSKG lable loading #####################################\n",
    "    def cskg_load(self,file_length=6003238): # cskg_connected.tsv\n",
    "        cskg_connected = self.cskg_connected\n",
    "        # create a dict to store cskg data set   label: node_list\n",
    "        # example : `p={'turtle':  ['Q1705322', '/c/en/turtle', ...], 'book`: [...]}`\n",
    "        cskg_index_dict = {}\n",
    "    \n",
    "        # create an inverted index to record lbl and node mapping\n",
    "        # example: p={'Q1705322': 'turtle', '/c/en/turtle': 'turtle'}\n",
    "        lbl_node_inv_index = {}\n",
    "\n",
    "        with open(cskg_connected) as f:\n",
    "            for line in tqdm(f,total=file_length):\n",
    "                content = line.split('\\t')\n",
    "                if content[0]!='id': # ignore the first time \n",
    "                    node1_id = content[1]\n",
    "                    node2_id = content[3]\n",
    "                    node1_lbl = content[4]\n",
    "                    node2_lbl = content[5]\n",
    "                    cskg_index_dict[node1_lbl] = cskg_index_dict.get(node1_lbl,set())\n",
    "                    cskg_index_dict[node1_lbl].add(node1_id)\n",
    "                    cskg_index_dict[node2_lbl] = cskg_index_dict.get(node2_lbl,set())\n",
    "                    cskg_index_dict[node2_lbl].add(node2_id)\n",
    "                    \n",
    "                    lbl_node_inv_index[node1_id] = node1_lbl\n",
    "                    lbl_node_inv_index[node2_id] = node2_lbl\n",
    "                    \n",
    "        # convert set to list\n",
    "        for k in cskg_index_dict:\n",
    "            cskg_index_dict[k] = list(cskg_index_dict[k])\n",
    "\n",
    "        return cskg_index_dict,lbl_node_inv_index\n",
    "    \n",
    "    ################# Util ###################################################\n",
    "    #Umapping txt to cskg => get common label's embeddings\n",
    "    def map_txt_cskg(self,text_embed_dict,lbl_node_inv_index):    \n",
    "        txt_lbl_emb_dict = {}\n",
    "        text_num = len(text_embed_dict)\n",
    "        for node in tqdm(text_embed_dict.keys(),total=text_num): \n",
    "            if node in lbl_node_inv_index:\n",
    "                label = lbl_node_inv_index[node]\n",
    "                txt_lbl_emb_dict[label] = text_embed_dict[node]\n",
    "            \n",
    "        return txt_lbl_emb_dict\n",
    "\n",
    "\"\"\"\n",
    "Class for data processing\n",
    "\"\"\"\n",
    "class DataProcesser():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_fassi_index(self,txt_lbl_emb_dict):\n",
    "        # txt_lbl_emb_dict is a dictionary which key is the node label and value is lable's embedding  \n",
    "        label_dict = {}         # build a entity label-index bi dictionary\n",
    "        entity_embeddings = []  # all the embeddings \n",
    "        index = 0\n",
    "        for key,value in txt_lbl_emb_dict.items():\n",
    "            label_dict[index] = key\n",
    "            label_dict[key] = index\n",
    "            index += 1    \n",
    "            entity_embeddings.append(value)\n",
    "\n",
    "        # entity_embeddings => matrix  X contains  all labels' embeddings \n",
    "        X = np.array(entity_embeddings).astype(np.float32)   # float32\n",
    "        dimension = X.shape[1]\n",
    "\n",
    "        # build index (METRIC_INNER_PRODUCT => cos )\n",
    "        vec_index = faiss.index_factory(dimension, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "        # normalize all vectors in order to get cos sim \n",
    "        faiss.normalize_L2(X)  \n",
    "        # add vectors to index\n",
    "        vec_index.add(X) \n",
    "        \n",
    "        return vec_index,label_dict,X\n",
    "\n",
    "    def create_queryset(self,USF_FAN_dict,label_dict,txt_lbl_emb_dict):\n",
    "        query_ent_vecs = []\n",
    "        query_ent_dict = {}\n",
    "        miss_concept = 0\n",
    "        miss_concept_list = []\n",
    "        \n",
    "        for key in USF_FAN_dict:\n",
    "            if key in txt_lbl_emb_dict:\n",
    "                query_ent_dict[len(query_ent_vecs)] = key\n",
    "                query_ent_dict[key] = len(query_ent_vecs)\n",
    "                query_ent_vecs.append(txt_lbl_emb_dict[key])\n",
    "            else:\n",
    "                miss_concept_list.append(key)\n",
    "                miss_concept+=1\n",
    "                \n",
    "        print(f'match label num from cskg and USF-FAN: {len(query_ent_vecs)}')\n",
    "        print(f'miss label num from cskg and USF-FAN: {miss_concept}, they are {miss_concept_list}')\n",
    "        query_ent_mat = np.array(query_ent_vecs).astype(np.float32)\n",
    "        faiss.normalize_L2(query_ent_mat) \n",
    "        return query_ent_mat,query_ent_dict\n",
    "   \n",
    "    def neighbor_searching(self,vec_index,query_ent_mat,query_ent_dict,label_dict,k,fix_num):\n",
    "        # k = times of items => k = 1 ,reterice @1 items k = 3 ,reterice @3 items \n",
    "        \n",
    "        neigh_num = k*fix_num\n",
    "        cos_sim, index = vec_index.search(query_ent_mat, neigh_num)     # both of them are matrices\n",
    "        \n",
    "        neighbors_dict = {}\n",
    "        for ix,neighbors in enumerate(index):\n",
    "            query_item = query_ent_dict[ix]\n",
    "            tmp_list = []\n",
    "            for id_ in neighbors:\n",
    "                tmp_list.append(label_dict[id_])            # ix refers to the label's index \n",
    "            \n",
    "            neighbors_dict[query_item] = tmp_list\n",
    "                    \n",
    "        return neighbors_dict   \n",
    "    \n",
    "    ### Evaluation\n",
    "    def apk(self,actual, predicted, k):   \n",
    "        # keep predicted's order igonore actual's order\n",
    "        if len(predicted)>k*len(actual):\n",
    "            predicted = predicted[:k]\n",
    "        ap = 0.0\n",
    "        num_hits = 0.0\n",
    "        for i,p in enumerate(predicted):\n",
    "            if p in actual and p not in predicted[:i]:\n",
    "                num_hits += 1.0\n",
    "                ap += num_hits / (i+1.0)\n",
    "                \n",
    "        if num_hits == 0:  # no match from predict and actual \n",
    "            return 0.0\n",
    "        else:\n",
    "            return ap / num_hits\n",
    "\n",
    "        \n",
    "    def map_at_k(self,pre_dict,grouding_dict,k):\n",
    "        MAP = 0 \n",
    "        set_size = len(pre_dict) \n",
    "\n",
    "        # cal ap\n",
    "        for label in pre_dict:\n",
    "            predicted = pre_dict.get(label,[])\n",
    "            actual = grouding_dict.get(label,[])\n",
    "            ap = self.apk(actual, predicted, k)\n",
    "            MAP+=ap\n",
    "\n",
    "        return MAP/set_size\n",
    "    \n",
    "    def reci_rank(self,actual, predicted):\n",
    "        # The inverse of the ranking of the first correct answer\n",
    "        # keep both predicted's order and actual's order\n",
    "        for i in  predicted:\n",
    "            if i in actual:\n",
    "                return 1/(actual.index(i)+1)\n",
    "            \n",
    "        return 0 # no match     \n",
    "    \n",
    "    def MPR(self,pre_dict,grouding_dict):\n",
    "        MPR = 0\n",
    "        set_size = len(pre_dict)\n",
    "        \n",
    "        for label in pre_dict:\n",
    "            predicted = pre_dict.get(label,[])\n",
    "            actual = grouding_dict.get(label,[])\n",
    "            rr = self.reci_rank(actual, predicted)\n",
    "            MPR+=rr\n",
    "            \n",
    "        return MPR/set_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader = DataLoader(cue_target,cskg_connected,bert_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'watch', 'alarm', 'tick', 'hands', 'work', 'radio']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. loda USF_FAN dataset\n",
    "USF_FAN_dict = dataLoader.xml_load()\n",
    "USF_FAN_dict['clock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6003238/6003238 [00:16<00:00, 358086.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2. load cskg dataset => here we only need its mapping of label and node\n",
    "cskg_index_dict,lbl_node_inv_index = dataLoader.cskg_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2161049/2161049 [11:31<00:00, 3124.37it/s] \n",
      "100%|██████████| 2161048/2161048 [00:02<00:00, 825309.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large text data nodes number: 2161048\n",
      "matched node number with cskg: 1522711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. text dataset load and mapping with cskg dataset\n",
    "txt_embed_dict = dataLoader.bert_load()\n",
    "\n",
    "txt_lbl_emb_dict = dataLoader.map_txt_cskg(txt_embed_dict,lbl_node_inv_index)\n",
    "print(f\"large text data nodes number: {len(txt_embed_dict)}\")\n",
    "print(f\"matched node number with cskg: {len(txt_lbl_emb_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataProcesser = DataProcesser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create vector index...\n",
      "CPU times: user 1min 29s, sys: 28.4 s, total: 1min 58s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 4. create vector index for bert-nli-large-embeddings by using fassi\n",
    "print('create vector index...')\n",
    "vec_index,label_dict,X = dataProcesser.build_fassi_index(txt_lbl_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create query entity matrix...\n",
      "match label num from cskg and USF-FAN: 5010\n",
      "miss label num from cskg and USF-FAN: 8, they are ['clorox', 'coca-cola', 'cornbeef', 'excedrin', 'grown-ups', 'head & shoulders', 'out fox', 'q-tips']\n"
     ]
    }
   ],
   "source": [
    "# 5. create query matrix bert-nli-large-embeddings \n",
    "print('create query entity matrix...')\n",
    "query_ent_mat,query_ent_dict = dataProcesser.create_queryset(USF_FAN_dict,label_dict,txt_lbl_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do searching for query entity matrix...\n",
      "['give up|personx give up', 'give up', 'gives up|personx gives up', 'gives up|person x gives up', 'gives up', 'giving up', 'gave up', 'to give up', 'give-up', 'have give up|have person y give up', 'giving-up', 'they give up', 'to give up ()|to give up (persony)', 'to give up on persony|to give up on', 'decide to give up', 'to have given up', 'i give up', 'pass up', 'gives up and stops|personx gives up and stops', 'to give up hope', 'drop out', 'to give it up', 'to drop out', 'to abandon him', 'give up place', 'disavaunce', 'give it up', 'die off', 'die away', 'give up looking', 'to quit', 'quit', 'disavaunces', 'cease'] ['quit', 'lose', 'give in', 'surrender', 'fail', 'loose', 'let go', 'forfeit', 'forget', 'never', 'quitter', 'stop', 'try', 'down', 'end', 'hope', 'hopeless', 'leave', 'loser', 'take']\n",
      "CPU times: user 19min 12s, sys: 1min 40s, total: 20min 53s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 6. search neighbors for matching items\n",
    "print('do searching for query entity matrix...')\n",
    "fix_num = dataLoader.actual_max_num # get the largest community num.\n",
    "\n",
    "# @1 neighbors\n",
    "k = 1\n",
    "neigbors_dict = dataProcesser.neighbor_searching(vec_index,query_ent_mat,query_ent_dict,label_dict,k,fix_num)\n",
    "print(neigbors_dict['give up'], USF_FAN_dict['give up'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@1 for predicted neigbors: 0.0013972055888223553\n"
     ]
    }
   ],
   "source": [
    "# 7. calculate the map for predicted neigbors (compared to USF-FAN)\n",
    "MAP = dataProcesser.map_at_k(neigbors_dict,USF_FAN_dict,k)\n",
    "print(f\"MAP@{k} for predicted neigbors: {MAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPR for predicted neigbors: 0.07282919576365023\n"
     ]
    }
   ],
   "source": [
    "#8. calculate the mrr for predicted neigbors (compared to USF-FAN)\n",
    "MPR = dataProcesser.MPR(neigbors_dict,USF_FAN_dict)\n",
    "print(f\"MPR for predicted neigbors: {MPR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@1 for predicted neigbors: 0.0013972055888223553\n",
      "MPR@1 for predicted neigbors: 0.07282919576365023\n",
      "\n",
      "\n",
      "MAP@2 for predicted neigbors: 0.014870259481037923\n",
      "MPR@2 for predicted neigbors: 0.09375124018944862\n",
      "\n",
      "\n",
      "MAP@3 for predicted neigbors: 0.020176314038589466\n",
      "MPR@3 for predicted neigbors: 0.11036673357141215\n",
      "\n",
      "\n",
      "MAP@5 for predicted neigbors: 0.025587159015302723\n",
      "MPR@5 for predicted neigbors: 0.1309352211403789\n",
      "\n",
      "\n",
      "MAP@10 for predicted neigbors: 0.03129357686742913\n",
      "MPR@10 for predicted neigbors: 0.1589292164519716\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all result\n",
    "ks = [1,2,3,5,10]\n",
    "for k in ks:\n",
    "    neigbors_dict = dataProcesser.neighbor_searching(vec_index,query_ent_mat,query_ent_dict,label_dict,k,fix_num)\n",
    "    MAP = dataProcesser.map_at_k(neigbors_dict,USF_FAN_dict,k)\n",
    "    MPR = dataProcesser.MPR(neigbors_dict,USF_FAN_dict)\n",
    "    print(f\"MAP@{k} for predicted neigbors: {MAP}\")\n",
    "    print(f\"MPR@{k} for predicted neigbors: {MPR}\") \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
