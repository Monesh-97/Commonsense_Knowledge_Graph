{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook for CSKG edge sentences embedding and clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clustering # a script contains related function for sentence embedding and clustering\n",
    "import importlib\n",
    "importlib.reload(clustering)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for invoking the notebook\n",
    "\n",
    "input\n",
    "\n",
    "- `cskg_connected`: file path of cskg_connected.tsv (contains the raw cskg edge information)\n",
    "- `cskg_connected_dim`: file path of cskg_connected_dim.tsv.gz（contains dimension-based (manual) clusters result）\n",
    "\n",
    "output:\n",
    "- `cskg_lexicalized`: file path of cskg_lexicalized.tsv (contains lexicalization of each edge on CSKG), each line has three item,edge_id,lexicalization, and sentence(separated by tab) \n",
    "- `edge_embeddings_bert`: file path of edge_embeddings_bert.tsv (contains edge id and its embeddings generated by sentence-transformer-bert on CSKG, not raw data but generated by methods )\n",
    "- `edge_embeddings_roberta`: file path of edge_embeddings_robert.tsv (contains edge id and its embeddings generated by sentence-transformer-roberta on CSKG, not raw data but generated by methods )\n",
    "\n",
    "- `clstr_bert`:  file path of clstr_bert.tsv（contains automatic clusters result by using sentence-transformer-bert model）\n",
    "- `clstr_roberta`: file path of clstr_bert.tsv（contains automatic clusters result by using  sentence-transformer-roberta model)\n",
    "\n",
    "- `log_bert`: a folder keeps tensorboard projector's configuration for bert sentence embedding and its predicted cluster labels\n",
    "- `log_roberta`: a folder keeps tensorboard projector's configuration for roberta sentence embedding and its predicted cluster labels\n",
    "- `log_bert_human`: a folder keeps tensorboard projector's configuration for bert sentence embedding and its cluster labels by human\n",
    "- `log_roberta_human`: a folder keeps tensorboard projector's configuration for roberta sentence embedding and its cluster labels by human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input\n",
    "cskg_connected = '../input/cskg_connected.tsv'\n",
    "cskg_connected_dim = '../input/cskg_connected_dim.tsv.gz'\n",
    "## output\n",
    "cskg_lexicalized = '../output/cskg_lexicalized.tsv'\n",
    "edge_embeddings_bert= '../output/edge_embeddings_bert.tsv'\n",
    "edge_embeddings_roberta = '../output/edge_embeddings_roberta.tsv'\n",
    "clstr_bert = '../output/clstr_bert.tsv'\n",
    "clstr_roberta = '../output/clstr_roberta.tsv'\n",
    "log_bert = '../output/log_bert'\n",
    "log_roberta = '../output/log_roberta'\n",
    "log_bert_human = '../output/log_bert_human'\n",
    "log_roberta_human = '../output/log_roberta_human'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  Datasets\n",
    "\n",
    "- `manually_res`:  A Dictionary whose key is the egde id, the value is the cluster label (manully)\n",
    "- `edge_list`:  A list contain multiple tuples kepping each edge's nodes and relation information each tuple's format is  (edge_id, node1_lbl, rel_lbl, node2_lbl, rel_meta)\n",
    "- `rel_dict`:  A dictionary whose key the relation ID , value keeps the relation label accoring to the relation ID. The value is also a dictionary whose key is the relation label, the value is the occurrence such relation label appears on CSKG <br> example: '/r/IsA': {'is a': 242358, 'subproperty of': 1, 'subclass of': 47501, 'instance of': 26685} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering label for edge  id '/c/en/0.22_inch_calibre-/r/IsA-/c/en/5.6_millimetres-0000': taxonomic\n",
      "\n",
      "edges on manually_res: 5822389\tcluster number: 12\n"
     ]
    }
   ],
   "source": [
    "## 0.load cluster result generated manually  (for the final comparison)\n",
    "manually_res = clustering.load_clstr_hand(cskg_connected_dim)\n",
    "print(f\"clustering label for edge  id '/c/en/0.22_inch_calibre-/r/IsA-/c/en/5.6_millimetres-0000': \\\n",
    "{manually_res['/c/en/0.22_inch_calibre-/r/IsA-/c/en/5.6_millimetres-0000']}\\n\")\n",
    "\n",
    "print(f\"edges on manually_res: {len(manually_res)}\",end='\\t')\n",
    "print(f\"cluster number: {len(set(list(manually_res.values())))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an edge on CSKG:('/c/en/0.22_inch_calibre-/r/IsA-/c/en/5.6_millimetres-0000', '0.22 inch calibre', 'is a', '5.6 millimetres', '/r/IsA')\n",
      "\n",
      "labels for relation ID '/r/IsA': {'is a': 242358, 'subproperty of': 1, 'subclass of': 47501, 'instance of': 26685}\n"
     ]
    }
   ],
   "source": [
    "## 1.load CSKG edges ,get each edge's nodes and relation info\n",
    "edge_list = clustering.get_edge(cskg_connected)\n",
    "rel_dict = clustering.rel_mapping(edge_list) \n",
    "\n",
    "print(f\"an edge on CSKG:{edge_list[0]}\")\n",
    "print()\n",
    "print(f\"labels for relation ID '/r/IsA': {rel_dict['/r/IsA']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8752 [('has', 16438), ('on', 27337), ('on a', 1341), ('in', 13550), ('of', 7047), ('of a', 1069), ('of an', 60), ('behind', 6378), ('facing', 97), ('carries', 66), ('full of', 211), ('of street', 2), ('attaches to', 2), ('black', 76), ('has a', 8677), ('doors on', 1), ('made of', 843), ('near', 2419), ('horses head', 1), ('attached to', 1417)]\n"
     ]
    }
   ],
   "source": [
    "### small finding:  Here we can see that the richest reltion ID is '/r/LocatedNear', having 8752 labels \n",
    "##((dive into more??))\n",
    "relation_types = sorted(rel_dict.items(),key=lambda x:len(x[1]),reverse=True)\n",
    "# here you can see even if the relation labels belong to one relation meta type, these descrpitiona are various\n",
    "print(len(relation_types[0][1]),list(relation_types[0][1].items())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create  lexicalization\n",
    "\n",
    "- `rel_template`: A dictionary made manually keeps the template for different relation types\n",
    "- `edge_sent_list`: A list contain multiple tuples kepping each edge's id, lexicalization and generated sentence each tuple's format is (edge_id, lexicalization, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel_template['/r/SimilarTo']: is similar to\n"
     ]
    }
   ],
   "source": [
    "## 2.get relation template and generate lexicalization for each edge\n",
    "rel_template = clustering.rel_template\n",
    "## create cskg_lexicalized.tsv for future usage\n",
    "edge_sent_list = clustering.create_lexi(edge_list,rel_template,cskg_lexicalized)\n",
    "print(f\"rel_template['/r/SimilarTo']: {clustering.rel_template['/r/SimilarTo']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence embedding\n",
    "\n",
    "- `sent_embs_bert`: A list contains multiple tupels, each tuple contians the edge's id and edge's sentence embedding generated by sentence-transformer-bert \n",
    "- `sent_embs_roberta`: A list contains multiple tupels, each tuple contians the edge's id and edge's sentence embedding generated by sentence-transformer-roberta\n",
    "\n",
    "\n",
    "We have storeed them into tsv  files for repetitive usage, so we can use load_sent_emb() function to import them\n",
    "while it still takes much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4053758/121448308839 [00:00<49:59, 40482314.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load sent_embs_bert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 121448284267/121448308839 [1:01:32<00:00, 32893585.61it/s]\n",
      "  0%|          | 4499978/120706669991 [00:00<44:53, 44813168.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of edge:5957575, dimension for each setence vec: 1024\n",
      "\n",
      "load sent_embs_roberta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 120706645419/120706669991 [1:00:39<00:00, 33161709.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of edge:5957575, dimension for each setence vec: 1024\n",
      "CPU times: user 1h 49min 4s, sys: 12min 47s, total: 2h 1min 51s\n",
      "Wall time: 2h 2min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 3. use two different pre-models to do sentence embedding （more than 10h!）\n",
    "# here we first check if the sentence embedding exists, if yes, then we load them\n",
    "# otherwise we will generate it by sentence transformer \n",
    "try:\n",
    "    print('load sent_embs_bert...')\n",
    "    sent_embs_bert = clustering.load_sent_emb(edge_embeddings_bert)\n",
    "    print(f\"# of edge:{len(sent_embs_bert)}, dimension for each setence vec: {len(sent_embs_bert[0][1])}\\n\")\n",
    "    print('load sent_embs_roberta...')\n",
    "    sent_embs_roberta = clustering.load_sent_emb(edge_embeddings_roberta) \n",
    "    print(f\"# of edge:{len(sent_embs_roberta)}, dimension for each setence vec: {len(sent_embs_roberta[0][1])}\")\n",
    "    \n",
    "except:\n",
    "    print('no existing files, now we generate them (takes more than 10hs!)')\n",
    "    sent_embs_bert = clustering.get_sent_emb('bert-large-nli-stsb-mean-tokens',edge_sent_list,edge_embeddings_bert)\n",
    "    sent_embs_roberta = ra.get_sent_emb('roberta-large-nli-stsb-mean-tokens',edge_sent_list,edge_embeddings_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Clustering by k-means\n",
    "- `clstr_res_bert` :A Dictionary whose key is the egde id, the value is the predicted cluster label by k-means and the embedding model is bert-large-nli-stsb-mean-tokens\n",
    "- `clstr_res_robert` :A Dictionary whose key is the egde id, the value is the predicted cluster label by k-means and the embedding model is roberta-large-nli-stsb-mean-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cluster result generated by bert model...\n",
      "cluster label for '/c/en/0.22_inch_calibre-/r/IsA-/c/en/5.6_millimetres-0000'    3\n",
      "load cluster result of roberta...\n",
      "cluster label for '/c/en/0.22_inch_calibre-/r/IsA-/c/en/5.6_millimetres-0000'    3\n",
      "CPU times: user 12.8 s, sys: 3.55 s, total: 16.3 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##4. According to the sentence embedding,  do clustering by k-means\n",
    "# here we first check if the sentence embedding exists, if yes, then we load them\n",
    "# otherwise we will generate it by their sentence embedding (To be honest, this step should \n",
    "# be proceeded before step3 if we can make sure that the cluster results are ready )\n",
    "try:\n",
    "    print('load cluster result generated by bert model...')\n",
    "    clstr_res_bert = clustering.load_clstr_auto(clstr_bert)\n",
    "    print(f\"cluster label for '/c/en/0.22_inch_calibre-/r/IsA-/c/en/5.6_millimetres-0000'\\\n",
    "    {clstr_res_bert['/c/en/0.22_inch_calibre-/r/IsA-/c/en/5.6_millimetres-0000']}\")\n",
    "    print('load cluster result of roberta...')\n",
    "    clstr_res_roberta = clustering.load_clstr_auto(clstr_roberta)\n",
    "    print(f\"cluster label for '/c/en/0.22_inch_calibre-/r/IsA-/c/en/5.6_millimetres-0000'\\\n",
    "    {clstr_res_roberta['/c/en/0.22_inch_calibre-/r/IsA-/c/en/5.6_millimetres-0000']}\")\n",
    "    \n",
    "except:\n",
    "    print('no existing files, now we generate them...')\n",
    "    clstr_res_bert = clustering.edge_cluster(sent_embs_bert,clstr_bert,cluster_num=13)\n",
    "    clstr_res_roberta = clustering.edge_cluster(sent_embs_roberta,cluster_num=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate adjusted rand index metric between antomatic result and human result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted rank index by using bert is:    0.22624362772679796\n",
      "adjusted rank index by using roberta is: 0.23544859881340863\n"
     ]
    }
   ],
   "source": [
    "ari_bert = clustering.adj_rank_index(clstr_res_bert,manually_res)\n",
    "ari_robert = clustering.adj_rank_index(clstr_res_roberta,manually_res)\n",
    "print(f\"adjusted rank index by using bert is:    {ari_bert}\")\n",
    "print(f\"adjusted rank index by using roberta is: {ari_robert}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize edge embddings \n",
    "\n",
    "We can use tensorboard projector to visualize edge embeddings\n",
    "After executing the following code, a log folder will be generated automatically, then using `tensorboard --logdir={log_folder}`to check visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_bert = '../output/log_bert'\n",
    "# log_roberta = '../output/log_roberta'\n",
    "# log_bert_human = '../output/log_bert_human'\n",
    "# log_roberta_human = '../output/log_roberta_human'\n",
    "# clustering. visualisation(clstr_res_bert, sent_embs_bert, log_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert embdding + human labels\n",
    "clustering.visualisation(manually_res, sent_embs_bert, log_bert_human,'bert_human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert embdding + auto labels\n",
    "clustering.visualisation(clstr_res_bert, sent_embs_bert, log_bert,'bert_auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta embdding + human labels\n",
    "clustering.visualisation(manually_res, sent_embs_roberta, log_roberta_human,'roberta_human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta embdding + auto labels\n",
    "clustering.visualisation(clstr_res_roberta, sent_embs_roberta, log_roberta,'roberta_auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
