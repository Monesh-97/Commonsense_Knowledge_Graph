{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "kgtk_wn_file=\"../tmp/kgtk_wordnet.tsv\"\n",
    "kgtk_cn_file=\"../tmp/kgtk_conceptnet.tsv\"\n",
    "\n",
    "# output files\n",
    "wn_gold_file=\"wn_gold_all.tsv\"\n",
    "wn_gold_200_file=\"wn_gold_200.tsv\"\n",
    "wn_mrs_prediction_file=\"wn_MRS_200.tsv\"\n",
    "wn_mfs_prediction_file=\"wn_MFS_200.tsv\"\n",
    "wn_stb_prediction_file=\"wn_STB_200.tsv\"\n",
    "wn_str_prediction_file=\"wn_STR_200.tsv\"\n",
    "cn_test_1k_file=\"cn_test_1k.tsv\"\n",
    "cn_prediction_file=\"cn_predict_1k.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate wn_gold_all.tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rltk\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "from itertools import combinations\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    with open(filename, \"r\",encoding=\"UTF-8\") as f:\n",
    "        \"\"\"\n",
    "        load data\n",
    "        token is split by \"\\t\"\n",
    "        \"\"\"\n",
    "        head = f.readline().strip().split(\"\\t\")\n",
    "        lines = []\n",
    "\n",
    "        for line in f:\n",
    "            lines.append(line.strip().split(\"\\t\"))\n",
    "    return head, lines\n",
    "head, lines = load_file(kgtk_wn_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the head look like\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_labels(node_labels,node_noid):\n",
    "    \"\"\"\n",
    "    Obtain label whose leven distance is most close to the node id\n",
    "    \"\"\"\n",
    "    node_res=[float(\"inf\"),\"\"]\n",
    "    for label in node_labels.split(\"|\"):\n",
    "        # remove \"\"\n",
    "        label = label.replace('\"',\"\").replace(\"\\\\'\",\"'\")\n",
    "        dis = rltk.levenshtein_distance(node_noid, label)\n",
    "        temp = [dis,label]\n",
    "        \n",
    "        if temp < node_res:\n",
    "            node_res  = temp\n",
    "            \n",
    "    return node_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gold_file(lines):\n",
    "    # Transfer line to head 'node1;label','relation','node2;label','node1','node2'\n",
    "    wn_gold_all = []\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        #change column to node1;label, relation, node2;label, node1, node2\n",
    "        node1_id = line[0]\n",
    "        relation = line[1]\n",
    "        node2_id = line[2]\n",
    "        node1_labels = line[3]\n",
    "        node2_labels = line[4]\n",
    "        # modeify the node labels, check with leve distance\n",
    "        \n",
    "        #node1_noid = node1.split(\":\")[1].split(\".\")[0]\n",
    "        #node2_noid = node2.split(\":\")[1].split(\".\")[0]\n",
    "\n",
    "        node1_label = multiple_labels(node1_labels,node1_id)\n",
    "        node2_label = multiple_labels(node2_labels,node2_id)\n",
    "\n",
    "        wn_gold_all.append([node1_label, relation, node2_label, node1_id, node2_id])\n",
    "        print(f\"\\r {i}/{len(lines)}\", end=\"\")\n",
    "        i += 1\n",
    "        \n",
    "    return wn_gold_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_gold_all = generate_gold_file(lines)\n",
    "# example of new dataset\n",
    "wn_gold_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new head\n",
    "new_head = ['node1;label','relation','node2;label','node1','node2']\n",
    "# write in to file\n",
    "with open(wn_gold_file, \"w\", newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow(new_head)\n",
    "    writer.writerows(wn_gold_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_gold_all[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(wn_gold_all):\n",
    "    # obtain the dsitribution of each label\n",
    "    # output: {label1-> str: num->integer}\n",
    "    distri = dict()\n",
    "    for line in wn_gold_all:\n",
    "        node1_label = line[0]\n",
    "        node2_label = line[2]\n",
    "        node1_id = line[3]\n",
    "        node2_id = line[4]\n",
    "        temp1 = distri.get(node1_label,set())\n",
    "        temp1.add(node1_id)\n",
    "        temp2 = distri.get(node2_label,set())\n",
    "        temp2.add(node2_id)\n",
    "        distri[node1_label] = temp1\n",
    "        distri[node2_label] = temp2\n",
    "    \n",
    "    for item in distri:\n",
    "        distri[item] = len(distri[item])\n",
    "    return distri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distri = distribution(wn_gold_all)\n",
    "plt.hist(distri.values(),log=True,bins=50)\n",
    "print(\"mean ambiguity of label:\", sum(distri.values())/len(distri), \"size of records:\", len(wn_gold_all), \"num of distinct labels:\", len(distri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in distri:\n",
    "    if distri[item] > 20:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random pick 200 records\n",
    "wn_gold_200 = random.choices(wn_gold_all, k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_gold_200[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_head = ['node1;label','relation','node2;label','node1','node2']\n",
    "# write in to file 200 records\n",
    "with open(wn_gold_200_file, \"w\", newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow(new_head)\n",
    "    writer.writerows(wn_gold_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synsets = list(wn.all_synsets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the regex structure of word combination\n",
    "regex_dis = dict()\n",
    "for synset in all_synsets:\n",
    "    name = synset.name().split(\".\")[0]\n",
    "    regex_temple = \"\"\n",
    "    for w in name:\n",
    "        if not regex_temple:\n",
    "            if w.isalpha():\n",
    "                regex_temple += \"x\"\n",
    "            else:\n",
    "                regex_temple += w\n",
    "                \n",
    "            continue\n",
    "        \n",
    "        if w.isalpha() and (not regex_temple[-1].isalpha()):\n",
    "            regex_temple += \"x\"\n",
    "            \n",
    "        elif not w.isalpha():\n",
    "            regex_temple += w\n",
    "    #if regex_temple == \"x_x_x_x'x_x\":\n",
    "    #   print(name)\n",
    "    regex_dis[regex_temple] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_ones(size, count):\n",
    "    for positions in combinations(range(size), count):\n",
    "        p = [0] * size\n",
    "\n",
    "        for i in positions:\n",
    "            p[i] = 1\n",
    "\n",
    "        yield p\n",
    "        \n",
    "# permutations of list n without repitation\n",
    "def permu(n):\n",
    "    comb = []\n",
    "    for i in range(n+1):\n",
    "        comb += place_ones(n,i)\n",
    "        \n",
    "    return comb\n",
    "\n",
    "def replace_str(string, replace_w, idx):\n",
    "    # replace whitespace to _ or -\n",
    "    return string[:idx] + replace_w +string[idx+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_words(label):\n",
    "    # accoording to the labels, generate label that can be recognized by wn interface from NLTK\n",
    "    idx_list = [x for x, v in enumerate(label) if v == ' ']\n",
    "    \n",
    "    if not idx_list:\n",
    "        #no whitespace in words\n",
    "        yield label\n",
    "    else:\n",
    "        # whitespace in words\n",
    "        combs = permu(len(idx_list))\n",
    "        for comb in combs:\n",
    "            for idx, status, in zip(idx_list, comb):\n",
    "                if status:\n",
    "                    label = replace_str(label, \"-\", idx)\n",
    "                else:\n",
    "                    label = replace_str(label, \"_\", idx)\n",
    "            yield label\n",
    "            \n",
    "def generate_synsets(labels):\n",
    "    # According to the generation of labels, obtain the synsets\n",
    "    for label in labels:\n",
    "        synsets = list(wn.synsets(label))\n",
    "        \n",
    "        if synsets:\n",
    "            return synsets, label\n",
    "        \n",
    "    return [], label\n",
    "\n",
    "def generate_candidates(label):\n",
    "    candidates,_ = generate_synsets(transfer_words(label))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of word combination\n",
    "generate_candidates(\"far cry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of word combination\n",
    "generate_candidates(\"ladder back\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets(\"far cry\"), wn.synsets(\"ladder back\"),wn.synsets(\"far_cry\"),wn.synsets(\"ladder-back\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRS(wn_gold):\n",
    "    # Random Baseline calculation\n",
    "    wn_predict = []\n",
    "    for line in wn_gold:\n",
    "        label1  = line[0]\n",
    "        label2 = line[2]\n",
    "        relationship = line[1]\n",
    "        \n",
    "        candidates1 = generate_candidates(label1)\n",
    "        candidates2 = generate_candidates(label2)\n",
    "        \n",
    "        if candidates1:\n",
    "            node1_id = random.choice(candidates1)\n",
    "        else:\n",
    "            print(label1)\n",
    "            #_ = label1\n",
    "            node1_id = \"\"\n",
    "        \n",
    "        if candidates2:\n",
    "            node2_id = random.choice(candidates2)\n",
    "        else:\n",
    "            print(label2)\n",
    "            #_ = label2\n",
    "            node2_id = \"\"\n",
    "        \n",
    "        #print(node2_id)\n",
    "        wn_predict.append([label1, relationship, label2, node1_id, node2_id])\n",
    "        \n",
    "    return wn_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_predict_200 = MRS(wn_gold_200)\n",
    "wn_predict_200[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synset2str(syn):\n",
    "    # transfer synset to str in gold file\n",
    "    # print(syn), print(type(syn))\n",
    "    if type(syn) == str:\n",
    "        return \"wn:\"\n",
    "    return \"wn:\"+ syn.name()\n",
    "\n",
    "def validation(wn_predict,wn_gold):\n",
    "    # check accracy of prediction\n",
    "    # accuracy1: if label is correct, true positive +1\n",
    "    # accuracy2: iff two labels in one record are correct (record is correct), true positive +1\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    for predict, actual in zip(wn_predict, wn_gold):\n",
    "        #print(predict, actual)\n",
    "        judge = [synset2str(predict[3]) == actual[3],synset2str(predict[4]) == actual[4]]\n",
    "        #print(predict[3],actual[3])\n",
    "        if judge[0]:\n",
    "            correct1 += 1\n",
    "            \n",
    "        if judge[1]:\n",
    "            correct1 += 1\n",
    "            \n",
    "        if all(judge):\n",
    "            correct2 += 1\n",
    "            \n",
    "    return correct1/(len(wn_predict)*2), correct2/len(wn_predict)\n",
    "\n",
    "def write_prediction(filename, lines):\n",
    "    # write prediction to file\n",
    "    new_head = ['node1;label','relation','node2;label','node1','node2']\n",
    "    # write in to file 200 records\n",
    "    with open(filename, \"w\", newline='',encoding=\"UTF-8\") as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow(new_head)\n",
    "        for line in lines:\n",
    "            writer.writerow([line[0],line[1],line[2],synset2str(line[3]),synset2str(line[4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_prediction(wn_mrs_prediction_file, wn_predict_200)\n",
    "accuracy1, accuracy2 = validation(wn_predict_200,wn_gold_200)\n",
    "accuracy1, accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFS(wn_gold):\n",
    "    # Frequent Baseline Calculation\n",
    "    wn_predict = []\n",
    "    for line in wn_gold:\n",
    "        label1  = line[0]\n",
    "        label2 = line[2]\n",
    "        relationship = line[1]\n",
    "        \n",
    "        candidates1 = generate_candidates(label1)\n",
    "        candidates2 = generate_candidates(label2)\n",
    "        \n",
    "        if candidates1:\n",
    "            node1_id = candidates1[0]\n",
    "        else:\n",
    "            #print(label1)\n",
    "            #_ = label1\n",
    "            node1_id = \"\"\n",
    "        \n",
    "        if candidates2:\n",
    "            node2_id = candidates2[0]\n",
    "        else:\n",
    "            #print(label2)\n",
    "            #_ = label2\n",
    "            node2_id = \"\"\n",
    "        \n",
    "        #print(node2_id)\n",
    "        wn_predict.append([label1, relationship, label2, node1_id, node2_id])\n",
    "        \n",
    "    return wn_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_predict_200 = MFS(wn_gold_200)\n",
    "wn_predict_200[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_prediction(wn_mfs_prediction_file, wn_predict_200)\n",
    "accuracy1, accuracy2 = validation(wn_predict_200,wn_gold_200)\n",
    "accuracy1, accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence-transformer-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_STB = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the relationship\n",
    "relationships = set()\n",
    "\n",
    "for line in wn_gold_all:\n",
    "    relationships.add(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2sentence = {'/r/IsA':\"is a\", '/r/MadeOf': \"is made of\",'/r/PartOf':\"is part of\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_sentence(line, word2sentence):\n",
    "    sentence = line[0]+\" \"+word2sentence[line[1]]+\" \"+line[2]\n",
    "    return sentence\n",
    "\n",
    "def label2sentence2embed(label,embeddings, model):\n",
    "    temp = []\n",
    "    if label in embeddings:\n",
    "        pass\n",
    "    else:\n",
    "        candidates = generate_candidates(label)\n",
    "\n",
    "        for candit in candidates:\n",
    "            temp.append([candit,model.encode(candit.definition())])\n",
    "            \n",
    "        embeddings[label] = temp\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "def candidates_embeddings(wn_gold, model):\n",
    "    embeddings = dict()\n",
    "    i = 0\n",
    "    for line in wn_gold:\n",
    "        label1  = line[0]\n",
    "        label2 = line[2]\n",
    "        print(\"\\r\",i, end=\"\")\n",
    "        i +=1\n",
    "        embeddings = label2sentence2embed(label1,embeddings, model)\n",
    "        embeddings = label2sentence2embed(label2,embeddings, model)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def STB(wn_gold, model = None, label_embeddings = None, word2sentence = None):\n",
    "    # sentence-transformer-bert Calculation\n",
    "    wn_predict = []\n",
    "    i = 0\n",
    "    for line in wn_gold:\n",
    "        label1 = line[0]\n",
    "        label2 = line[2]\n",
    "        sentence=line_sentence(line, word2sentence)\n",
    "        sent_embedding = model.encode(sentence)\n",
    "\n",
    "        #obtain the max similar item for label1\n",
    "        max_item = [0,\"\"]\n",
    "        if label1 not in label_embeddings:\n",
    "            node_id1 = \"\"\n",
    "        else:\n",
    "            for candit, embedding in label_embeddings[label1]:\n",
    "                similar = rltk.cosine_similarity(list(embedding), list(sent_embedding))\n",
    "                temp = [similar, candit]\n",
    "\n",
    "                if temp>max_item:\n",
    "                    max_item = temp\n",
    "\n",
    "            node_id1 = max_item[-1]\n",
    "        \n",
    "        #obtain the max similar item for label2\n",
    "        max_item = [0,\"\"]\n",
    "        if label2 not in label_embeddings:\n",
    "            node_id2 = \"\"\n",
    "        else:\n",
    "            for candit, embedding in label_embeddings[label2]:\n",
    "                similar = rltk.cosine_similarity(list(embedding), list(sent_embedding))\n",
    "                temp = [similar, candit]\n",
    "\n",
    "                if temp>max_item:\n",
    "                    max_item = temp\n",
    "\n",
    "            node_id2 = max_item[-1]\n",
    "        print(\"\\r\",i, end=\"\")\n",
    "        i +=1\n",
    "                \n",
    "        wn_predict.append([label1, line[1], label2,node_id1,node_id2])\n",
    "        \n",
    "    return wn_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embeddings = candidates_embeddings(wn_gold_200, model_STB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_predict_200 = STB(wn_gold_200, model = model_STB, label_embeddings = label_embeddings, word2sentence = word2sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_prediction(wn_stb_prediction_file, wn_predict_200)\n",
    "accuracy1, accuracy2 = validation(wn_predict_200,wn_gold_200)\n",
    "accuracy1, accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence-transformer-roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_STR = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STR(wn_gold, model = None, label_embeddings = label_embeddings, word2sentence = word2sentence):\n",
    "    return STB(wn_gold, model = model, label_embeddings = label_embeddings, word2sentence = word2sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embeddings = candidates_embeddings(wn_gold_200, model_STR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_predict_200 = STR(wn_gold_200, model = model_STR, label_embeddings = label_embeddings, word2sentence = word2sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_prediction(wn_str_prediction_file, wn_predict_200)\n",
    "accuracy1, accuracy2 = validation(wn_predict_200,wn_gold_200)\n",
    "accuracy1, accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_MRS = []\n",
    "accuracy_MFS = []\n",
    "accuracy_STB = []\n",
    "accuracy_STR = []\n",
    "n=10\n",
    "for i in range(n):\n",
    "    print(i)\n",
    "    wn_gold_200 = random.choices(wn_gold_all, k=200)\n",
    "    wn_predict_200 = MRS(wn_gold_200)\n",
    "    accuracy1, accuracy2 = validation(wn_predict_200,wn_gold_200)\n",
    "    accuracy_MRS.append(accuracy1)\n",
    "    \n",
    "    wn_predict_200 = MFS(wn_gold_200)\n",
    "    accuracy1, accuracy2 = validation(wn_predict_200,wn_gold_200)\n",
    "    accuracy_MFS.append(accuracy1)\n",
    "    \n",
    "    label_embeddings = candidates_embeddings(wn_gold_200, model_STB)\n",
    "    wn_predict_200 = STB(wn_gold_200, model = model_STB, label_embeddings = label_embeddings, word2sentence = word2sentence)\n",
    "    accuracy1, accuracy2 = validation(wn_predict_200,wn_gold_200)\n",
    "    accuracy_STB.append(accuracy1)\n",
    "    \n",
    "    label_embeddings = candidates_embeddings(wn_gold_200, model_STR)\n",
    "    wn_predict_200 = STR(wn_gold_200, model = model_STR, label_embeddings = label_embeddings, word2sentence = word2sentence)\n",
    "    accuracy1, accuracy2 = validation(wn_predict_200,wn_gold_200)\n",
    "    accuracy_STR.append(accuracy1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "x_axis = range(n)\n",
    "plt.plot(x_axis, accuracy_MRS, color='green', label='MRS')\n",
    "plt.plot(x_axis, accuracy_MFS, color='red', label='MFS')\n",
    "plt.plot(x_axis, accuracy_STB,  color='skyblue', label='STB')\n",
    "plt.plot(x_axis, accuracy_STR, color='blue', label='STR')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('iteration times')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head, lines = load_file(kgtk_cn_file)\n",
    "lines_1k = random.choices(lines, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_test_all = generate_gold_file(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_test_1k = generate_gold_file(lines_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_head = ['node1;label','relation','node2;label','node1','node2']\n",
    "# write in to file 200 records\n",
    "with open(cn_test_1k_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow(new_head)\n",
    "    writer.writerows(cn_test_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_predict_1k = MFS(cn_test_1k)\n",
    "write_prediction(cn_prediction_file, cn_predict_1k)\n",
    "cn_predict_1k[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Mehotd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_synset_prob(cn_predict_1k):\n",
    "    #prob 1: no synset for label\n",
    "    #prob 2: no synset for record\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for record in cn_predict_1k:\n",
    "        judge = [synset2str(record[3]) == \"wn:\",synset2str(record[4]) == \"wn:\"]\n",
    "        \n",
    "        if judge[0]:\n",
    "            count1 += 1\n",
    "            \n",
    "        if judge[1]:\n",
    "            count1 += 1\n",
    "            \n",
    "        if any(judge):\n",
    "            count2 += 1\n",
    "            \n",
    "    return count1/(2*len(cn_predict_1k)), count2/len(cn_predict_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1,prob2 = no_synset_prob(cn_predict_1k)\n",
    "prob1, prob2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence-transformer-roberta for WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embeddings = candidates_embeddings(cn_test_1k, model_STR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = set()\n",
    "\n",
    "for line in lines:\n",
    "    relationships.add(line[1])\n",
    "    \n",
    "relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2sentence = {'/r/Antonym':\"is antonym for\", \n",
    "                 '/r/AtLocation': \"is located at\",\n",
    "                 '/r/CapableOf':\"is capable of\",\n",
    "                '/r/Causes':\"causes\",\n",
    "                '/r/CausesDesire':\"causes the desire of\",\n",
    "                '/r/CreatedBy':\"is created by\",\n",
    "                '/r/DefinedAs': \" is defined as\",\n",
    "                '/r/DerivedFrom': \"is derived from\",\n",
    "                '/r/Desires':\"desires\",\n",
    "                '/r/DistinctFrom':\"is distinct from\",\n",
    "                \"/r/Entails\":\"entails\",\n",
    "                '/r/EtymologicallyDerivedFrom':\"is etymologically derived from\",\n",
    "                '/r/EtymologicallyRelatedTo': \"is etymologically related to\",\n",
    "                '/r/FormOf':\"is form of\",\n",
    "                '/r/HasA': \"has a\",\n",
    "                '/r/HasContext': \"has the context of\",\n",
    "                '/r/HasFirstSubevent': \"has first subevent, \",\n",
    "                '/r/HasLastSubevent':\"has last subevent, \",\n",
    "                '/r/HasPrerequisite': \"has prerequisite, \",\n",
    "                '/r/HasProperty': \"has property, \",\n",
    "                '/r/HasSubevent': \"has subevent, \",\n",
    "                '/r/InstanceOf': \" is an instance of\",\n",
    "                '/r/IsA': \"is a\",\n",
    "                '/r/LocatedNear': \"is located nearby\",\n",
    "                '/r/MadeOf': \"is made of\",\n",
    "                '/r/MannerOf':\"has a manner of\",\n",
    "                '/r/MotivatedByGoal': \"is motivated by goal\",\n",
    "                '/r/NotCapableOf': \"is not capable of\",\n",
    "                '/r/NotDesires':\"does not desire\",\n",
    "                '/r/NotHasProperty':\"does not have property, \",\n",
    "                '/r/PartOf': \"is part of\",\n",
    "                '/r/ReceivesAction':\"receives the action, \",\n",
    "                '/r/RelatedTo':\"is related to\",\n",
    "                '/r/SimilarTo':\"is similar to\",\n",
    "                '/r/SymbolOf':\"is a symbol of\",\n",
    "                '/r/Synonym':\"is synonym for\",\n",
    "                '/r/UsedFor':\"is used for\",\n",
    "                '/r/dbpedia/capital': \"is the capital of\",\n",
    "                '/r/dbpedia/field':\" is the field of\",\n",
    "                '/r/dbpedia/genre':\"has genre,\",\n",
    "                '/r/dbpedia/genus':\"has genus, \",\n",
    "                '/r/dbpedia/influencedBy':\"is influenced by\",\n",
    "                '/r/dbpedia/knownFor': \"is known for\",\n",
    "                '/r/dbpedia/language':\"is the language \",\n",
    "                '/r/dbpedia/leader':\"has the leader, \",\n",
    "                '/r/dbpedia/occupation':\"has the occupation, \",\n",
    "                '/r/dbpedia/product':\"has the product, \"}\n",
    "\n",
    "cn_predict_1k = STR(cn_test_1k, model = model_STR, label_embeddings = label_embeddings, word2sentence = word2sentence)\n",
    "write_prediction(cn_prediction_file, cn_predict_1k)\n",
    "prob1,prob2 = no_synset_prob(cn_predict_1k)\n",
    "prob1, prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distri = distribution(cn_test_1k)\n",
    "plt.hist(distri.values(),log=True,bins=30)\n",
    "print(\"mean ambiguity of label:\", sum(distri.values())/len(distri), \"size of records:\", len(cn_test_1k),\"num of distinct labels:\", len(distri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
