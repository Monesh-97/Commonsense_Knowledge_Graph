{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall process of Faiss can be divided into three steps:\n",
    "\n",
    "1. Construct training data (expressed in matrix form)\n",
    "2. Select the appropriate Index (the core component of Faiss) and add the training data to the Index.\n",
    "3. Search, that is, search, get the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libaray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construct training data (expressed in matrix form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 4.97 s, total: 1min 12s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# specify a certain entity embedding tsv file\n",
    "input_file = '/nas/home/binzhang/backup_data/complex/comp_log_dot_0.01/entities_output.tsv'\n",
    "entity_dict = {}        # build a entity name-index bi dictionary\n",
    "entity_embeddings = []  # all the embeddings \n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    for index,line in enumerate(f):\n",
    "        line = line.split('\\t')\n",
    "        entity_name = line[0]\n",
    "        entity_vec =  [ float(i) for i in line[1:]]\n",
    "        entity_embeddings.append(entity_vec)\n",
    "        entity_dict[entity_name] = index\n",
    "        entity_dict[index] = entity_name\n",
    "        \n",
    "# entity_embeddings=> matrix\n",
    "X = np.array(entity_embeddings).astype(np.float32) # float32\n",
    "dimension = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select the appropriate Index (cos) and add the training data to the Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector number in index: 2160968\n"
     ]
    }
   ],
   "source": [
    "# build index (METRIC_INNER_PRODUCT => cos )\n",
    "vec_index = faiss.index_factory(dimension, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "# # normalize all vectors in order to get cos sim \n",
    "faiss.normalize_L2(X)  \n",
    "# add vectors to inde \n",
    "vec_index.add(X) \n",
    "print(f'vector number in index: {vec_index.ntotal}')# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Do some searching \n",
    "\n",
    "\n",
    "normal case:<br>\n",
    "query_set = [[...],[...],[...]]<br>\n",
    "query_mat = np.array([dataSetII]).astype(np.float32)<br>\n",
    "faiss.normalize_L2(query_mat) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first I mimic some query data from the training data\n",
    "query_ent_indices = list(range(0,10)) # first 10 entities\n",
    "query_ent_vecs = [] \n",
    "for i in query_ent_indices:\n",
    "    query_ent_vecs.append(X[i])\n",
    "query_ent_mat = np.array(query_ent_vecs)\n",
    "faiss.normalize_L2(query_ent_mat) \n",
    "query_ent_mat.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity by FAISS:\n",
      " [[1.         0.9759227  0.9747515  0.97438985 0.97427124]\n",
      " [1.         0.99250054 0.99220634 0.9920102  0.9919175 ]\n",
      " [1.         0.9914187  0.9910581  0.99104    0.99095213]\n",
      " [1.         0.9887444  0.9884101  0.9883417  0.98831266]\n",
      " [1.         0.9605595  0.94857997 0.9483327  0.94539094]\n",
      " [1.         0.98139644 0.9810555  0.9809668  0.9807242 ]\n",
      " [0.99999994 0.9594297  0.957194   0.95686686 0.9543254 ]\n",
      " [1.         0.9819611  0.9794824  0.9794055  0.9789906 ]\n",
      " [1.0000001  0.9560226  0.9528179  0.95084345 0.94829893]\n",
      " [0.99999994 0.99290884 0.99220634 0.9917029  0.99170125]]\n",
      "Index by FAISS:\n",
      " [[      0 1807674 1904214  530173 1037121]\n",
      " [      1 1816434  578516 1134567 1126372]\n",
      " [      2  594499 1403122  353072 2105736]\n",
      " [      3  901349  808479  939125  866220]\n",
      " [      4 1045313  943524  528468 1606929]\n",
      " [      5 1677129  695595 1347835  168479]\n",
      " [      6 1388697   49499  746061  212139]\n",
      " [      7 1612105 1411100 1779663  658361]\n",
      " [      8 1944259  236965 1784814 1052320]\n",
      " [      9 2046695 1666816  647275 1460659]]\n"
     ]
    }
   ],
   "source": [
    "# after setting topk ,we can do query\n",
    "topk = 5\n",
    "cos_sim, index = vec_index.search(query_ent_mat, topk) # both of them are matrices\n",
    "print(f'Similarity by FAISS:\\n {cos_sim}')\n",
    "print(f'Index by FAISS:\\n {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result \n",
    "res = []\n",
    "for row in range(len(index)):\n",
    "    top5_res = []\n",
    "    for col in range(len(index[0])):\n",
    "        ent_name = entity_dict[index[row,col]]\n",
    "        sim = cos_sim[row,col]\n",
    "        top5_res.append((ent_name,sim))\n",
    "    res.append(top5_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/c/en/one_drives', 1.0),\n",
       " ('/c/en/scoring_homer_ball', 0.9759227),\n",
       " ('/c/en/coffee_spilled', 0.9747515),\n",
       " ('/c/en/computer_switched_on', 0.97438985),\n",
       " ('/c/en/people_love_away', 0.97427124)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/c/en/nip_outs/n', 1.0),\n",
       " ('/c/en/cenogenetic/a', 0.99250054),\n",
       " ('/c/en/banghyangs/n', 0.99220634),\n",
       " ('/c/en/louques/n', 0.9920102),\n",
       " ('/c/en/dataplanes/n', 0.9919175)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('/c/en/one_drives', 1.0),\n",
       "  ('/c/en/scoring_homer_ball', 0.9759227),\n",
       "  ('/c/en/coffee_spilled', 0.9747515),\n",
       "  ('/c/en/computer_switched_on', 0.97438985),\n",
       "  ('/c/en/people_love_away', 0.97427124)],\n",
       " [('/c/en/nip_outs/n', 1.0),\n",
       "  ('/c/en/cenogenetic/a', 0.99250054),\n",
       "  ('/c/en/banghyangs/n', 0.99220634),\n",
       "  ('/c/en/louques/n', 0.9920102),\n",
       "  ('/c/en/dataplanes/n', 0.9919175)],\n",
       " [('/c/en/waycasters/n', 1.0),\n",
       "  ('/c/en/rosemans/n', 0.9914187),\n",
       "  ('/c/en/ballantines/n', 0.9910581),\n",
       "  ('/c/en/futchels/n', 0.99104),\n",
       "  ('/c/en/holdaways/n', 0.99095213)],\n",
       " [('at:to_buy_a_similar_shirt', 1.0),\n",
       "  ('at:to_continue_working_until_the_end_of_the_day', 0.9887444),\n",
       "  ('at:to_relax/lounge/sleep', 0.9884101),\n",
       "  ('at:to_stand_ground', 0.9883417),\n",
       "  ('at:to_stay_lost', 0.98831266)],\n",
       " [('/c/en/eagar', 1.0),\n",
       "  ('/c/en/entrapped', 0.9605595),\n",
       "  ('/c/en/nostalgiac', 0.94857997),\n",
       "  ('/c/en/resistent', 0.9483327),\n",
       "  ('at:nervous_as_they_attempt_something_new', 0.94539094)],\n",
       " [('/c/en/vesperate', 1.0),\n",
       "  ('/c/en/phenomenalise', 0.98139644),\n",
       "  ('/c/en/geologise', 0.9810555),\n",
       "  ('/c/en/raunge', 0.9809668),\n",
       "  ('/c/en/herse', 0.9807242)],\n",
       " [('/c/en/favorableness/n/wn/attribute', 0.99999994),\n",
       "  ('/c/en/excitation/n/wn/state', 0.9594297),\n",
       "  ('/c/en/barrage/n/wn/act', 0.957194),\n",
       "  ('/c/en/richweed/n/wn/plant', 0.95686686),\n",
       "  ('/c/en/softness/n/wn/state', 0.9543254)],\n",
       " [('/c/en/irenical/a', 1.0),\n",
       "  ('/c/en/pushful/a', 0.9819611),\n",
       "  ('/c/en/hated/a', 0.9794824),\n",
       "  ('/c/en/life_affirming/a', 0.9794055),\n",
       "  ('/c/en/have_way_with/v', 0.9789906)],\n",
       " [('/c/en/except_in_eke_out', 1.0000001),\n",
       "  ('/c/en/used_to_reassure_or_comfort_person_to_whom_it_is_said', 0.9560226),\n",
       "  ('/c/en/regional_obsolete', 0.9528179),\n",
       "  ('/c/en/restaurant_slang', 0.95084345),\n",
       "  ('/c/en/after_noun_modified', 0.94829893)],\n",
       " [('/c/en/connecting_hose', 0.99999994),\n",
       "  ('/c/en/pouring_water', 0.99290884),\n",
       "  ('/c/en/looking_cool_out_in_ocean', 0.99220634),\n",
       "  ('/c/en/turning_water_on_and_off', 0.9917029),\n",
       "  ('/c/en/filling_water_balloons', 0.99170125)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
