{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSKG embeddings\n",
    "\n",
    "This notebook computes similarity between nodes in CSKG and performs grounding of questions/answers to CSKG.\n",
    "\n",
    "We will play with two different families of embeddings: graph and text embeddings.\n",
    "\n",
    "## Graph embeddings \n",
    "\n",
    "The graph embeddings have been computed by the command:\n",
    "\n",
    "`python embeddings/embedding_click.py -i input/kgtk_framenet.tsv -o output/kgtk_framenet`\n",
    "\n",
    "using the `embedding/embedding_click.py` script in this repository. This command invokes the Facebook PyBigGraph (PBG) library and computes graph embeddings with the ComplEx algorithm.\n",
    "\n",
    "We are currently integrating this function into the KGTK package, to make it more accessible to the AI community.\n",
    "\n",
    "## Text embeddings\n",
    "The text embeddings were computed by using the KGTK `text-embedding` command as follows:\n",
    "```\n",
    "kgtk text_embedding \\\n",
    "    --embedding-projector-metadata-path none \\\n",
    "    --label-properties \"label\" \\\n",
    "    --isa-properties \"/r/IsA\" \\\n",
    "    --description-properties \"/r/DefinedAs\" \\\n",
    "    --property-value \"/r/Causes\" \"/r/UsedFor\" \"/r/PartOf\" \"/r/AtLocation\" \"/r/CapableOf\" \\\n",
    "    \"/r/CausesDesire\" \"/r/SymbolOf\" \"/r/MadeOf\" \"/r/LocatedNear\" \"/r/Desires\" \"/r/HasProperty\" \"/r/HasFirstSubevent\" \\\n",
    "    \"/r/HasLastSubevent\" \"at:xAttr\" \"at:xEffect\" \"at:xIntent\" \"at:xNeed\" \"at:xReact\" \"at:xWant\" \\\n",
    "    --has-properties \"\" \\\n",
    "    -f kgtk_format \\\n",
    "    --output-data-format kgtk_format \\\n",
    "    --model bert-large-nli-cls-token \\\n",
    "    --save-embedding-sentence \\\n",
    "    -i sorted.tsv.gz \\\n",
    "    -p sorted.tsv.gz \\\n",
    "    > cskg_embedings.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for grounding\n",
    "\n",
    "```\n",
    "conda create -n mowgli-env python=3.6 \n",
    "conda activate mowgli-env\n",
    "\n",
    "git clone https://github.com/ucinlp/mowgli-uci\n",
    "\n",
    "mv mowgli-uci grounding\n",
    "\n",
    "cd grounding\n",
    "\n",
    "pip install -r requirements.txt\n",
    "conda install --yes faiss-cpu -c pytorch -n mowgli-env\n",
    "python -m spacy download en_core_web_lg\n",
    "\n",
    "cd ..\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for working with embeddings\n",
    "\n",
    "`conda install -c conda-forge python-annoy`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Callable, List, Tuple\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERAL CONFIG\n",
    "redo=True\n",
    "CACHE_DIR = Path('.cache/')\n",
    "\n",
    "## ANNOY SETUP\n",
    "distance='cosine'\n",
    "\n",
    "## GRAPH EMBEDDINGS SETUP\n",
    "graph_dim=400 # Dimension of the graph embeddings - choose one of 100, 300, 400\n",
    "graph_trees=20\n",
    "graph_emb_path='output/embeddings/entity_embedding_%d.tsv.gz' % graph_dim\n",
    "graph_index_path='tmp/complex_%d.ann' % graph_dim\n",
    "graph_node2id_path='tmp/graph_node2id.pkl'\n",
    "graph_id2node_path='tmp/graph_id2node.pkl'\n",
    "graph_emb_col=1\n",
    "graph_emb_del=' '\n",
    "\n",
    "\n",
    "## TEXT EMBEDDINGS SETUP\n",
    "text_dim=1024\n",
    "text_trees=10\n",
    "text_emb_path='output/embeddings/cskg_embeddings_bert_nli_large.txt.gz'\n",
    "text_index_path='tmp/bert_large.ann'\n",
    "text_node2id_path='tmp/text_node2id.pkl'\n",
    "text_id2node_path='tmp/text_id2node.pkl'\n",
    "text_emb_col=2\n",
    "text_emb_del=','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cache():\n",
    "    if not CACHE_DIR.exists():\n",
    "        logger.debug(f'Creating cache dir at: {CACHE_DIR}')\n",
    "        CACHE_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cache_path(fn, args, kwargs):\n",
    "    fn_name = fn.__name__\n",
    "    args_string = ','.join(str(arg) for arg in args)\n",
    "    kwargs_string = json.dumps(kwargs)\n",
    "    byte_string = fn_name + args_string + kwargs_string\n",
    "    hash_object = hashlib.sha1(byte_string.encode())\n",
    "    return CACHE_DIR / hash_object.hexdigest()\n",
    "\n",
    "\n",
    "def cache():\n",
    "    def decorator(fn):\n",
    "        def load_cached_if_available(*args, **kwargs):\n",
    "            path = _cache_path(fn, args, kwargs)\n",
    "            if path.exists():\n",
    "                logger.debug(f'Loading `{fn.__name__}` output from cache')\n",
    "                with open(path, 'rb') as f:\n",
    "                    return pkl.load(f)\n",
    "            output = fn(*args, **kwargs)\n",
    "            with open(path, 'wb') as f:\n",
    "                pkl.dump(output, f, protocol=4)\n",
    "            return output\n",
    "        return load_cached_if_available\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(metric: str, embeddings: np.ndarray):\n",
    "\n",
    "    logger.debug(f'Building search index')\n",
    "\n",
    "    if metric == 'cosine':\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[-1])\n",
    "    elif metric == 'l2':\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[-1])\n",
    "    else:\n",
    "        raise ValueError(f'Bad metric: {metric}')\n",
    "\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, words) -> None:\n",
    "        self.idx_to_word = words\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines(embedding_file: Path):\n",
    "    with gzip.open(embedding_file, 'r') as f:\n",
    "        i=0\n",
    "        for line in f:\n",
    "#            if 'embedding_sentence' in line_data: continue\n",
    "            i+=1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache()\n",
    "def read_embedding_file(embedding_file: Path, dim: int, emb_col=1) -> Tuple[Vocab, np.ndarray]:\n",
    "\n",
    "    logger.debug(f'Reading embeddings from {embedding_file}')\n",
    "\n",
    "    shape = tuple([count_lines(embedding_file), dim])\n",
    "                  \n",
    "    with gzip.open(embedding_file, 'r') as f:\n",
    "\n",
    "        embeddings = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "        if emb_col!=1:\n",
    "            header=next(f)\n",
    "        i=0\n",
    "        words = []\n",
    "        for line in tqdm(f, total=shape[0]):\n",
    "            line=line.decode()\n",
    "            if emb_col==1:\n",
    "                node, *embedding = line.split()\n",
    "            else:\n",
    "                line_data=line.split()\n",
    "                if line_data[1]=='embedding_sentence': continue\n",
    "                node=line_data[0]\n",
    "                embedding=line_data[2].split(',')\n",
    "            embedding = np.array([float(x) for x in embedding])\n",
    "            words.append(node)\n",
    "            embeddings[i] = embedding\n",
    "            i+=1\n",
    "\n",
    "    vocab = Vocab(words)\n",
    "\n",
    "    return vocab, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2160968/2160968 [05:08<00:00, 7003.03it/s]\n"
     ]
    }
   ],
   "source": [
    "graph_vocab, graph_embeddings = read_embedding_file(graph_emb_path, graph_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_index = build_index(distance, graph_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 4322096/4322097 [12:14<00:00, 5882.91it/s]\n"
     ]
    }
   ],
   "source": [
    "text_vocab, text_embeddings = read_embedding_file(text_emb_path, text_dim, emb_col=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index = build_index(distance, text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Most similar nodes in CSKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_nodes=['/c/en/turtle', '/c/en/happy', '/c/en/turtle/n/wn/animal', 'at:personx_abandons_____altogether', '/c/en/caffeine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbors=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors to */c/en/turtle*\n",
      "['/c/en/tortoise', '/c/en/animal', '/c/en/carapace', '/c/en/bill/n/wikt/en_2', '/c/en/turtled/v']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/happy*\n",
      "['/c/en/joyful', '/c/en/excited', '/c/en/pleased', '/c/en/glad', '/c/en/elated']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/turtle/n/wn/animal*\n",
      "['/c/en/luger/n/wn/person', '/c/en/mud_turtle/n/wn/animal', '/c/en/carapace/n/wn/animal', '/c/en/chelonian/n/wn/animal', '/c/en/testudinidae/n/wn/animal']\n",
      "\n",
      "\n",
      "Nearest neighbors to *at:personx_abandons_____altogether*\n",
      "['at:to_start_fresh', '/c/en/sad', '/c/en/impatient', 'at:to_find_a_new_job', '/c/en/authoritative']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/caffeine*\n",
      "['/c/en/coffee', '/c/en/caffeinated/a', '/c/en/caffeine_free', '/c/en/tea', '/c/en/drug/n']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids=[graph_vocab.word_to_idx[n] for n in query_nodes]\n",
    "distances, neighbors = graph_index.search(graph_embeddings[ids], num_neighbors+1)\n",
    "for node_nbrs in neighbors:\n",
    "    neighboring_nodes=[graph_vocab.idx_to_word[n] for n in node_nbrs]\n",
    "\n",
    "    print('Nearest neighbors to *%s*' % neighboring_nodes[0])\n",
    "    print(neighboring_nodes[1:])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors to */c/en/turtle*\n",
      "['/c/en/turtles', '/c/en/large_sea_turtle/n', '/c/en/large_freshwater_turtle/n', '/c/en/shrimp_and_turtle', '/c/en/sea_turtle/n']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/happy*\n",
      "['at:to_tell_personx_they_are_happy_to_see_that', 'at:to_tell_personx_they_are_happy_to_see_them', 'at:tell_people_how_happy_they_are', 'at:to_let_person_x_know_how_happy_they_are', '/c/en/bring_happiness']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/turtle/n/wn/animal*\n",
      "['Q1705322', '/c/en/sea_turtle/n/wn/animal', '/c/en/freshwater_turtle/n', '/c/en/sea_turtle/n', '/c/en/ridley_sea_turtle']\n",
      "\n",
      "\n",
      "Nearest neighbors to *at:personx_abandons_____altogether*\n",
      "[\"at:personx_loses_personx's_ability\", 'at:personx_chases_persony_away', 'at:personx_goes_too_far', 'at:personx_feels_hopeless', 'at:personx_loses_persony_opportunity']\n",
      "\n",
      "\n",
      "Nearest neighbors to */c/en/caffeine*\n",
      "['at:wakes_up_from_caffeine', 'at:becomes_more_awake_from_the_caffeine', '/c/en/amphetamines', 'at:to_be_energized_with_caffeine', '/c/en/caffeine_which_raises_brain_reaction_time']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids=[text_vocab.word_to_idx[n] for n in query_nodes]\n",
    "\n",
    "distances, neighbors = text_index.search(text_embeddings[ids], num_neighbors+1)\n",
    "\n",
    "for node_nbrs in neighbors:\n",
    "    neighboring_nodes=[text_vocab.idx_to_word[n] for n in node_nbrs]\n",
    "\n",
    "    print('Nearest neighbors to *%s*' % neighboring_nodes[0])\n",
    "    print(neighboring_nodes[1:])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Compute similarity between two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pairs=[['/c/en/woman', '/c/en/man'], ['/c/en/pencil', 'Q614304'], ['/c/en/ash', 'rg:en_ash-gray'], ['/c/en/spiritual', '/c/en/religion'], ['/c/en/monkey', '/c/en/gorilla'], ['/c/en/monkey', '/c/en/tea']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/en/woman /c/en/man 0.35791242\n",
      "/c/en/pencil Q614304 -0.083074115\n",
      "/c/en/ash rg:en_ash-gray 0.11169712\n",
      "/c/en/spiritual /c/en/religion 0.2867519\n",
      "/c/en/monkey /c/en/gorilla 0.32852727\n",
      "/c/en/monkey /c/en/tea -0.004493271\n"
     ]
    }
   ],
   "source": [
    "for nodes in node_pairs:\n",
    "    ids=[graph_vocab.word_to_idx[n] for n in nodes]\n",
    "    ge=graph_embeddings[ids]\n",
    "    print(' '.join(nodes), cosine_similarity([ge[0]], [ge[1]])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/en/woman /c/en/man 0.66569746\n",
      "/c/en/pencil Q614304 0.36885476\n",
      "/c/en/ash rg:en_ash-gray 0.35797378\n",
      "/c/en/spiritual /c/en/religion 0.5916587\n",
      "/c/en/monkey /c/en/gorilla 0.6783559\n",
      "/c/en/monkey /c/en/tea 0.6468366\n"
     ]
    }
   ],
   "source": [
    "for nodes in node_pairs:\n",
    "    ids=[text_vocab.word_to_idx[n] for n in nodes]\n",
    "    ge=text_embeddings[ids]\n",
    "    print(' '.join(nodes), cosine_similarity([ge[0]], [ge[1]])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Parsing questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grounding.graphify import parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[\n",
    "    'Max looked for the onions so that he could  make a stew.',\n",
    "    'To get the bathroom counters dry after washing your face, take a small hand lotion and wipe away the extra water around the sink.',\n",
    "    'To get the bathroom counters dry after washing your face, take a small hand towel and wipe away the extra water around the sink.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_trees=parse.graphify_dataset(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent_data in parse_trees:\n",
    "    print('Sentence:', sent_data['sentence'])\n",
    "    print('Tokenized sentence', sent_data['tokenized_sentence'])\n",
    "    \n",
    "    nodes={}\n",
    "    for n_id, n_data in sent_data['nodes'].items():\n",
    "        nodes[n_id]=n_data['phrase']\n",
    "    \n",
    "    for e_id, e_data in sent_data['edges'].items():\n",
    "        print('NODE1:', ' '.join(nodes[e_data['head_node_id']]), 'RELATION', e_data['edge_name'], 'NODE2', ' '.join(nodes[e_data['tail_node_id']]) )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Grounding questions and questions to ConceptNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grounding.graphify import link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_data=link.link(parse_trees, embedding_file='grounding/numberbatch-en-19.08.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent_data in linked_data:\n",
    "    print('Sentence:', sent_data['sentence'])\n",
    "    for n_id, n_data in sent_data['nodes'].items():\n",
    "        print('Node phrase:', n_data['phrase'])\n",
    "        for c in reversed(n_data['candidates']):\n",
    "            print(c)\n",
    "        print()\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
